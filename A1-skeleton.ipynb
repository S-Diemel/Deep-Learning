{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1 2AMM10 2023-2024\n",
    "\n",
    "## Group: [Fill in your group name]\n",
    "### Member 1: [Fill in your name]\n",
    "### Member 2: [Fill in your name]\n",
    "### Member 3: [Fill in your name]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc4f04c033b0e00"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "import time\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function for loading the training data:\n",
    "\n",
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    This function loads the data from the specified pickle file and returns a dictionary with the data\n",
    "    :param filename: the pickle file\n",
    "    :return: dict with data -- keys and values differ for the train data and test data for each task.\n",
    "     Please see the cells with example code below for explanations and examples of the data structure per data set.\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19b9de0e3461531",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = load_data('train_data.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0da60a825f5080b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example alphabet names: ['Alphabet_of_the_Magi', 'Anglo-Saxon_Futhorc', 'Arcadian', 'Armenian', 'Asomtavruli_(Georgian)']\n",
      "\n",
      "\n",
      "how to get an example image for a specific character:\n",
      "shape of image 2 of character character06 of alphabet Asomtavruli_(Georgian): torch.Size([1, 105, 105])\n"
     ]
    }
   ],
   "source": [
    "# the structure of the training data is a dict, where the keys are strings indicating the alphabet.\n",
    "# The values are again dicts, with the keys being the character and the values being a list of images of that character.\n",
    "\n",
    "# see the code below for examples of working with the train data\n",
    "\n",
    "alphabets = list(train_data.keys())\n",
    "\n",
    "\n",
    "print('example alphabet names:', alphabets[:5])\n",
    "print('\\n')\n",
    "print('how to get an example image for a specific character:')\n",
    "\n",
    "alphabet_id = 4\n",
    "alphabet = alphabets[alphabet_id]  # a dict\n",
    "characters_for_this_alphabet = list(train_data[alphabet].keys())\n",
    "character_id = 5\n",
    "character = characters_for_this_alphabet[character_id]\n",
    "image_id = 2\n",
    "\n",
    "print(f'shape of image {image_id} of character {character} of alphabet {alphabet}:', train_data[alphabet][character][image_id].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289b9d9817ddf745",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function for plotting some examples:\n",
    "def plot_example_data(data_dict):\n",
    "    \"\"\"\n",
    "    This function plots some examples of the data\n",
    "    :param data_dict: dict with as keys a string specifying the alphabet, and as values a dict with as keys the character of the alphabet, and as values a list om images of the alphabet\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    alphabets_to_plot = np.random.choice(list(data_dict.keys()), size=10, replace=False)\n",
    "    \n",
    "    for i, alphabet in enumerate(alphabets_to_plot):\n",
    "        characters = data_dict[alphabet]\n",
    "        character_to_plot = np.random.choice(list(characters.keys()), size=1)[0]\n",
    "        images = characters[character_to_plot]\n",
    "        im_idx = np.random.choice(len(images), size=1)[0]\n",
    "        axs[i//5, i%5].imshow(images[im_idx].permute(1, 2, 0))\n",
    "        axs[i//5, i%5].set_title(alphabet + '\\n' + character_to_plot, fontsize=8)\n",
    "        axs[i//5, i%5].axis('off')\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68bfac0812b8988",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x432 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plot_example_data(train_data)\n",
    "# plt.savefig('example_data.png', dpi=600)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "511e9fbc1b85e80f",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: character recognition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6449bef2185716"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Alphabet_of_the_Magi': 20,\n 'Anglo-Saxon_Futhorc': 29,\n 'Arcadian': 26,\n 'Armenian': 41,\n 'Asomtavruli_(Georgian)': 40,\n 'Balinese': 24,\n 'Bengali': 46,\n 'Blackfoot_(Canadian_Aboriginal_Syllabics)': 14,\n 'Braille': 26,\n 'Burmese_(Myanmar)': 34,\n 'Cyrillic': 33,\n 'Early_Aramaic': 22,\n 'Futurama': 26,\n 'Grantha': 43,\n 'Greek': 24,\n 'Gujarati': 48,\n 'Hebrew': 22,\n 'Inuktitut_(Canadian_Aboriginal_Syllabics)': 16,\n 'Japanese_(hiragana)': 52,\n 'Japanese_(katakana)': 47,\n 'Korean': 40,\n 'Latin': 26,\n 'Malay_(Jawi_-_Arabic)': 40,\n 'Mkhedruli_(Georgian)': 41,\n 'N_Ko': 33,\n 'Ojibwe_(Canadian_Aboriginal_Syllabics)': 14,\n 'Sanskrit': 42,\n 'Syriac_(Estrangelo)': 23,\n 'Tagalog': 17,\n 'Tifinagh': 55}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_lengths = {}\n",
    "for i in range(len(list(train_data.keys()))):\n",
    "    alphabet = alphabets[i]  # a dict\n",
    "    characters_for_this_alphabet = list(train_data[alphabet].keys())\n",
    "    alphabet_lengths[alphabet] = len(characters_for_this_alphabet)\n",
    "alphabet_lengths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f78ab6a6133991a4",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Alphabet_of_the_Magi', 'Anglo-Saxon_Futhorc', 'Arcadian', 'Armenian', 'Asomtavruli_(Georgian)', 'Balinese', 'Bengali', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Braille', 'Burmese_(Myanmar)', 'Cyrillic', 'Early_Aramaic', 'Futurama', 'Grantha', 'Greek', 'Gujarati', 'Hebrew', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Japanese_(hiragana)', 'Japanese_(katakana)', 'Korean', 'Latin', 'Malay_(Jawi_-_Arabic)', 'Mkhedruli_(Georgian)', 'N_Ko', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Sanskrit', 'Syriac_(Estrangelo)', 'Tagalog', 'Tifinagh'])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['N_Ko']['character01'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Alphabet_of_the_Magi': <torch.utils.data.dataloader.DataLoader at 0x2b1cead6708>,\n 'Anglo-Saxon_Futhorc': <torch.utils.data.dataloader.DataLoader at 0x2b1c02dcd48>,\n 'Arcadian': <torch.utils.data.dataloader.DataLoader at 0x2b1ceb2e308>,\n 'Armenian': <torch.utils.data.dataloader.DataLoader at 0x2b1ceb65fc8>,\n 'Asomtavruli_(Georgian)': <torch.utils.data.dataloader.DataLoader at 0x2b1cebc6b48>,\n 'Balinese': <torch.utils.data.dataloader.DataLoader at 0x2b1cec20408>,\n 'Bengali': <torch.utils.data.dataloader.DataLoader at 0x2b1cec5f0c8>,\n 'Blackfoot_(Canadian_Aboriginal_Syllabics)': <torch.utils.data.dataloader.DataLoader at 0x2b1cedd6648>,\n 'Braille': <torch.utils.data.dataloader.DataLoader at 0x2b1cedf9348>,\n 'Burmese_(Myanmar)': <torch.utils.data.dataloader.DataLoader at 0x2b1cee32fc8>,\n 'Cyrillic': <torch.utils.data.dataloader.DataLoader at 0x2b1cee85608>,\n 'Early_Aramaic': <torch.utils.data.dataloader.DataLoader at 0x2b1ceecf308>,\n 'Futurama': <torch.utils.data.dataloader.DataLoader at 0x2b1cef01d48>,\n 'Grantha': <torch.utils.data.dataloader.DataLoader at 0x2b1cef3f648>,\n 'Greek': <torch.utils.data.dataloader.DataLoader at 0x2b1cefa0f08>,\n 'Gujarati': <torch.utils.data.dataloader.DataLoader at 0x2b1cefd8fc8>,\n 'Hebrew': <torch.utils.data.dataloader.DataLoader at 0x2b1cf048dc8>,\n 'Inuktitut_(Canadian_Aboriginal_Syllabics)': <torch.utils.data.dataloader.DataLoader at 0x2b1cf07f2c8>,\n 'Japanese_(hiragana)': <torch.utils.data.dataloader.DataLoader at 0x2b1cf0a1cc8>,\n 'Japanese_(katakana)': <torch.utils.data.dataloader.DataLoader at 0x2b1cf11e0c8>,\n 'Korean': <torch.utils.data.dataloader.DataLoader at 0x2b1cf186ec8>,\n 'Latin': <torch.utils.data.dataloader.DataLoader at 0x2b1cf1e7448>,\n 'Malay_(Jawi_-_Arabic)': <torch.utils.data.dataloader.DataLoader at 0x2b1d2b4edc8>,\n 'Mkhedruli_(Georgian)': <torch.utils.data.dataloader.DataLoader at 0x2b1d2baf2c8>,\n 'N_Ko': <torch.utils.data.dataloader.DataLoader at 0x2b1d2c09588>,\n 'Ojibwe_(Canadian_Aboriginal_Syllabics)': <torch.utils.data.dataloader.DataLoader at 0x2b1d2c5c0c8>,\n 'Sanskrit': <torch.utils.data.dataloader.DataLoader at 0x2b1fd397e48>,\n 'Syriac_(Estrangelo)': <torch.utils.data.dataloader.DataLoader at 0x2b245669c88>,\n 'Tagalog': <torch.utils.data.dataloader.DataLoader at 0x2b246671ac8>,\n 'Tifinagh': <torch.utils.data.dataloader.DataLoader at 0x2b24863a488>}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dict = {f\"character{i:02d}\": i - 1 for i in range(1, 100)}\n",
    "\n",
    "batch_size = 20\n",
    "train_loader_dict = {}\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets:\n",
    "    data_alphabet = train_data[alphabet]\n",
    "    image_label_list = []\n",
    "    for label, images in data_alphabet.items():\n",
    "        # Append each image-label pair as a tuple to the list\n",
    "        for image in images:\n",
    "            image_label_list.append((image, char_dict[label]))\n",
    "    train_loader = torch.utils.data.DataLoader(image_label_list, batch_size=batch_size, shuffle=True)\n",
    "    train_loader_dict[alphabet] = train_loader\n",
    "train_loader_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Batch 0:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([20, 10, 10, 18, 10,  3,  1,  7, 11,  5,  3, 17,  1, 14,  4,  6, 25, 20,\n",
      "         5, 20])\n",
      "Batch 1:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 2, 15,  4, 17,  1, 14, 23, 24,  2, 19, 23,  9, 15, 14, 15,  0, 11, 16,\n",
      "        10, 21])\n",
      "Batch 2:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([11,  4, 14, 23, 18, 19, 10,  6, 15,  6, 12, 16, 11,  8,  2, 20, 16, 20,\n",
      "        24,  6])\n",
      "Batch 3:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([22, 14, 25,  7, 11, 11, 11, 23,  3,  6, 25,  3, 13, 19, 11, 15, 25, 21,\n",
      "        20, 22])\n",
      "Batch 4:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([23,  9, 19,  7, 14, 10, 25,  0,  4,  9,  9, 23, 21, 25, 23, 12, 24, 12,\n",
      "         5, 13])\n",
      "Batch 5:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([15, 15,  8,  3,  5,  1,  5, 21, 24,  7,  2,  6, 24, 13,  3, 19, 17,  3,\n",
      "         2, 22])\n",
      "Batch 6:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([16,  9,  3,  2, 24, 25, 21, 21, 13,  3, 25, 15, 15,  0,  4,  6, 22, 22,\n",
      "        17, 17])\n",
      "Batch 7:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([20,  7, 23, 20, 16, 19, 22, 22, 13, 14,  7,  9, 12,  8, 24, 25, 12,  7,\n",
      "        13, 20])\n",
      "Batch 8:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([17, 21, 25, 15,  3,  0, 10,  7, 17, 15,  4,  3, 11, 23,  3,  1,  2, 21,\n",
      "        23, 19])\n",
      "Batch 9:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([19,  5,  0,  6, 16, 18, 24,  8, 19, 19,  6, 13,  3, 21,  4,  3, 13,  4,\n",
      "         2, 22])\n",
      "Batch 10:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([18,  5,  1,  9,  9,  8,  0,  0,  5,  2,  1,  4, 24, 19,  1, 12,  5,  7,\n",
      "        11, 17])\n",
      "Batch 11:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 0, 12, 14, 19, 10, 16,  0,  4,  6, 11, 16,  5, 19, 19,  4, 24,  3, 17,\n",
      "        21,  2])\n",
      "Batch 12:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([24, 10, 12, 18,  5, 19, 18, 12, 16, 14, 15,  9,  3, 18, 20, 10,  0, 10,\n",
      "         1,  8])\n",
      "Batch 13:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([10,  6, 21, 21,  9,  4, 12,  8,  7, 13, 22,  2, 16, 23, 22, 16, 11, 14,\n",
      "        18,  8])\n",
      "Batch 14:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([18, 22, 24, 22,  9, 14, 25, 11, 19, 22,  7,  4,  6, 17, 14, 17, 18, 15,\n",
      "        23,  8])\n",
      "Batch 15:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 6,  1,  3,  0,  2, 14, 23,  3,  5,  7, 19, 16, 19, 13, 12, 13, 25, 25,\n",
      "         0, 21])\n",
      "Batch 16:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 8, 18,  0,  5, 13, 24,  8, 13, 18, 12,  6,  8, 17,  6,  9, 25, 16, 16,\n",
      "        22,  8])\n",
      "Batch 17:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 9,  9, 20,  4, 24, 22, 16, 25,  4,  2,  3, 21, 22, 20, 19,  1, 25, 13,\n",
      "         4,  8])\n",
      "Batch 18:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([17,  2, 17, 12, 14, 24, 15,  0,  2, 13,  0,  2, 11, 20,  9, 12, 22,  1,\n",
      "        21,  2])\n",
      "Batch 19:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 5,  1, 21,  5,  2,  1,  5, 14, 13, 15, 10,  5, 21, 24,  7,  0,  6, 12,\n",
      "         8, 16])\n",
      "Batch 20:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([23, 25,  8, 15,  5, 20, 18, 10, 19,  1, 14, 18, 20, 13,  6, 14,  0, 17,\n",
      "        24,  4])\n",
      "Batch 21:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([ 8, 16, 23, 10, 17, 23,  5, 20, 10, 18, 11,  2, 11, 10, 11,  0,  7, 21,\n",
      "        11, 20])\n",
      "Batch 22:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([18, 25, 18, 20, 24,  0,  5, 23,  9,  1,  9, 14, 22,  1, 23, 16, 15, 22,\n",
      "        20,  7])\n",
      "Batch 23:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([23, 14, 13, 23,  9, 17,  1, 20, 16, 17,  9,  7,  4, 11,  0,  9,  4,  1,\n",
      "         4, 12])\n",
      "Batch 24:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([15,  7, 11, 18,  1, 12, 21, 15, 21,  8, 12, 17,  2, 18, 25,  3, 12,  7,\n",
      "        10,  6])\n",
      "Batch 25:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "Targets (labels):\n",
      "tensor([13, 14,  8, 18,  6,  6, 16, 24, 12, 25, 22,  7,  8, 24, 10, 13, 17, 15,\n",
      "        10,  7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_loader_dict['Latin']))\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader_dict['Latin']):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Inputs (features):\")\n",
    "    print(type(inputs))\n",
    "    print(inputs)  # Print input data (features)\n",
    "    print(\"Targets (labels):\")\n",
    "    print(targets)  # Print target data (labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * 26 * 26, 64)  # Adjusted based on input image size of 105x105\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class BaseCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseCNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 13 * 13, 512)  # Adjusted based on input image size of 105x105\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class BaseCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseCNN3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 26 * 26, 128)  # Adjusted based on input image size of 105x105\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, alphabet_num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_cnn = BaseCNN()\n",
    "        self.alphabet_classifiers = nn.ModuleDict({\n",
    "            alphabet: nn.Linear(64, num_classes) for alphabet, num_classes in alphabet_num_classes.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base_cnn(x)\n",
    "        outputs = {alphabet: classifier(features) for alphabet, classifier in self.alphabet_classifiers.items()}\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "948e55a5ea036e8a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the custom model\n",
    "model = CustomModel(alphabet_lengths)\n",
    "model.to(device)\n",
    "# Define loss functions and optimizer\n",
    "loss_functions = {alphabet: nn.CrossEntropyLoss() for alphabet in alphabet_lengths}\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.862 | Acc: 13.000% (52/400) | Conf 9.83 | time (s): 12.95 -- Alphabet_of_the_Magi -- \n",
      "Loss: 3.137 | Acc: 11.724% (68/580) | Conf 11.90 | time (s): 13.92 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 3.320 | Acc: 4.615% (24/520) | Conf 6.31 | time (s): 14.79 -- Arcadian -- \n",
      "Loss: 3.639 | Acc: 5.732% (47/820) | Conf 4.19 | time (s): 16.27 -- Armenian -- \n",
      "Loss: 3.743 | Acc: 4.125% (33/800) | Conf 3.90 | time (s): 17.70 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 3.153 | Acc: 6.458% (31/480) | Conf 5.43 | time (s): 18.66 -- Balinese -- \n",
      "Loss: 3.800 | Acc: 3.587% (33/920) | Conf 3.86 | time (s): 20.26 -- Bengali -- \n",
      "Loss: 2.562 | Acc: 12.857% (36/280) | Conf 10.65 | time (s): 20.80 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 3.227 | Acc: 7.692% (40/520) | Conf 6.64 | time (s): 21.76 -- Braille -- \n",
      "Loss: 3.532 | Acc: 6.324% (43/680) | Conf 4.65 | time (s): 22.91 -- Burmese_(Myanmar) -- \n",
      "Loss: 3.472 | Acc: 4.545% (30/660) | Conf 4.59 | time (s): 24.15 -- Cyrillic -- \n",
      "Loss: 3.103 | Acc: 6.818% (30/440) | Conf 7.34 | time (s): 24.93 -- Early_Aramaic -- \n",
      "Loss: 3.169 | Acc: 7.885% (41/520) | Conf 8.10 | time (s): 25.86 -- Futurama -- \n",
      "Loss: 3.668 | Acc: 6.047% (52/860) | Conf 5.45 | time (s): 27.43 -- Grantha -- \n",
      "Loss: 3.155 | Acc: 5.833% (28/480) | Conf 6.31 | time (s): 28.41 -- Greek -- \n",
      "Loss: 3.799 | Acc: 4.271% (41/960) | Conf 4.37 | time (s): 30.21 -- Gujarati -- \n",
      "Loss: 3.133 | Acc: 6.364% (28/440) | Conf 6.52 | time (s): 31.13 -- Hebrew -- \n",
      "Loss: 2.769 | Acc: 4.062% (13/320) | Conf 7.39 | time (s): 31.75 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 3.929 | Acc: 2.885% (30/1040) | Conf 2.93 | time (s): 33.51 -- Japanese_(hiragana) -- \n",
      "Loss: 3.842 | Acc: 3.298% (31/940) | Conf 2.97 | time (s): 35.11 -- Japanese_(katakana) -- \n",
      "Loss: 3.629 | Acc: 4.750% (38/800) | Conf 4.63 | time (s): 36.49 -- Korean -- \n",
      "Loss: 3.267 | Acc: 9.615% (50/520) | Conf 5.74 | time (s): 37.43 -- Latin -- \n",
      "Loss: 3.627 | Acc: 6.000% (48/800) | Conf 4.18 | time (s): 39.14 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 3.751 | Acc: 2.317% (19/820) | Conf 3.87 | time (s): 40.59 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 3.483 | Acc: 5.000% (33/660) | Conf 3.73 | time (s): 42.10 -- N_Ko -- \n",
      "Loss: 2.595 | Acc: 7.500% (21/280) | Conf 11.07 | time (s): 42.75 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 3.761 | Acc: 2.857% (24/840) | Conf 3.39 | time (s): 44.22 -- Sanskrit -- \n",
      "Loss: 3.127 | Acc: 6.087% (28/460) | Conf 5.69 | time (s): 45.15 -- Syriac_(Estrangelo) -- \n",
      "Loss: 2.640 | Acc: 15.588% (53/340) | Conf 13.19 | time (s): 45.78 -- Tagalog -- \n",
      "Loss: 3.990 | Acc: 4.182% (46/1100) | Conf 3.09 | time (s): 47.70 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 3.4023124320625766, TOTAL ACCURACY: 5.658713692946058 %\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.741 | Acc: 16.000% (64/400) | Conf 13.09 | time (s): 0.99 -- Alphabet_of_the_Magi -- \n",
      "Loss: 2.772 | Acc: 23.793% (138/580) | Conf 22.10 | time (s): 2.28 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 3.299 | Acc: 9.423% (49/520) | Conf 10.01 | time (s): 3.47 -- Arcadian -- \n",
      "Loss: 3.149 | Acc: 15.244% (125/820) | Conf 14.94 | time (s): 4.99 -- Armenian -- \n",
      "Loss: 3.351 | Acc: 12.125% (97/800) | Conf 11.86 | time (s): 6.48 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 3.316 | Acc: 9.583% (46/480) | Conf 11.16 | time (s): 7.36 -- Balinese -- \n",
      "Loss: 3.724 | Acc: 3.587% (33/920) | Conf 4.15 | time (s): 9.06 -- Bengali -- \n",
      "Loss: 2.166 | Acc: 28.571% (80/280) | Conf 25.54 | time (s): 9.54 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 2.632 | Acc: 26.923% (140/520) | Conf 22.52 | time (s): 10.61 -- Braille -- \n",
      "Loss: 3.042 | Acc: 18.529% (126/680) | Conf 20.23 | time (s): 11.79 -- Burmese_(Myanmar) -- \n",
      "Loss: 3.079 | Acc: 16.818% (111/660) | Conf 13.56 | time (s): 13.20 -- Cyrillic -- \n",
      "Loss: 2.935 | Acc: 18.182% (80/440) | Conf 17.30 | time (s): 14.01 -- Early_Aramaic -- \n",
      "Loss: 2.819 | Acc: 19.038% (99/520) | Conf 18.56 | time (s): 14.96 -- Futurama -- \n",
      "Loss: 3.137 | Acc: 15.233% (131/860) | Conf 16.17 | time (s): 16.59 -- Grantha -- \n",
      "Loss: 2.856 | Acc: 18.333% (88/480) | Conf 16.16 | time (s): 17.55 -- Greek -- \n",
      "Loss: 3.352 | Acc: 11.562% (111/960) | Conf 13.73 | time (s): 19.47 -- Gujarati -- \n",
      "Loss: 2.883 | Acc: 13.182% (58/440) | Conf 16.62 | time (s): 20.29 -- Hebrew -- \n",
      "Loss: 2.364 | Acc: 27.188% (87/320) | Conf 20.54 | time (s): 20.98 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 3.203 | Acc: 16.442% (171/1040) | Conf 17.14 | time (s): 22.85 -- Japanese_(hiragana) -- \n",
      "Loss: 3.314 | Acc: 15.532% (146/940) | Conf 14.25 | time (s): 24.49 -- Japanese_(katakana) -- \n",
      "Loss: 2.656 | Acc: 31.125% (249/800) | Conf 26.74 | time (s): 25.95 -- Korean -- \n",
      "Loss: 2.825 | Acc: 24.231% (126/520) | Conf 22.52 | time (s): 26.91 -- Latin -- \n",
      "Loss: 2.877 | Acc: 20.125% (161/800) | Conf 19.93 | time (s): 28.63 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 3.429 | Acc: 12.927% (106/820) | Conf 11.36 | time (s): 30.15 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 2.809 | Acc: 25.758% (170/660) | Conf 19.97 | time (s): 31.51 -- N_Ko -- \n",
      "Loss: 2.284 | Acc: 28.929% (81/280) | Conf 36.34 | time (s): 32.08 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 3.329 | Acc: 12.738% (107/840) | Conf 12.49 | time (s): 33.72 -- Sanskrit -- \n",
      "Loss: 2.759 | Acc: 20.652% (95/460) | Conf 18.95 | time (s): 34.58 -- Syriac_(Estrangelo) -- \n",
      "Loss: 2.051 | Acc: 35.588% (121/340) | Conf 34.38 | time (s): 35.21 -- Tagalog -- \n",
      "Loss: 2.540 | Acc: 32.545% (358/1100) | Conf 29.77 | time (s): 37.32 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 3.0889269555876364, TOTAL ACCURACY: 18.433609958506224 %\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.018 | Acc: 41.500% (166/400) | Conf 42.04 | time (s): 0.70 -- Alphabet_of_the_Magi -- \n",
      "Loss: 1.748 | Acc: 45.862% (266/580) | Conf 47.08 | time (s): 1.85 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 2.246 | Acc: 37.308% (194/520) | Conf 34.86 | time (s): 2.94 -- Arcadian -- \n",
      "Loss: 2.028 | Acc: 41.829% (343/820) | Conf 42.22 | time (s): 4.45 -- Armenian -- \n",
      "Loss: 1.582 | Acc: 52.500% (420/800) | Conf 52.93 | time (s): 5.98 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 2.941 | Acc: 19.583% (94/480) | Conf 22.10 | time (s): 6.90 -- Balinese -- \n",
      "Loss: 2.723 | Acc: 24.783% (228/920) | Conf 23.55 | time (s): 8.56 -- Bengali -- \n",
      "Loss: 1.154 | Acc: 62.500% (175/280) | Conf 59.20 | time (s): 9.22 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 1.900 | Acc: 48.846% (254/520) | Conf 46.46 | time (s): 10.13 -- Braille -- \n",
      "Loss: 2.252 | Acc: 33.382% (227/680) | Conf 36.75 | time (s): 11.32 -- Burmese_(Myanmar) -- \n",
      "Loss: 1.741 | Acc: 47.879% (316/660) | Conf 44.66 | time (s): 12.49 -- Cyrillic -- \n",
      "Loss: 1.682 | Acc: 48.182% (212/440) | Conf 49.55 | time (s): 13.32 -- Early_Aramaic -- \n",
      "Loss: 1.614 | Acc: 50.769% (264/520) | Conf 47.63 | time (s): 14.30 -- Futurama -- \n",
      "Loss: 2.272 | Acc: 35.814% (308/860) | Conf 34.14 | time (s): 15.86 -- Grantha -- \n",
      "Loss: 1.480 | Acc: 52.708% (253/480) | Conf 54.39 | time (s): 16.74 -- Greek -- \n",
      "Loss: 2.202 | Acc: 34.167% (328/960) | Conf 38.23 | time (s): 18.50 -- Gujarati -- \n",
      "Loss: 1.676 | Acc: 46.136% (203/440) | Conf 44.86 | time (s): 19.36 -- Hebrew -- \n",
      "Loss: 1.044 | Acc: 63.750% (204/320) | Conf 65.37 | time (s): 19.93 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 1.719 | Acc: 50.962% (530/1040) | Conf 49.50 | time (s): 21.92 -- Japanese_(hiragana) -- \n",
      "Loss: 1.814 | Acc: 47.128% (443/940) | Conf 46.74 | time (s): 23.68 -- Japanese_(katakana) -- \n",
      "Loss: 1.338 | Acc: 60.500% (484/800) | Conf 60.96 | time (s): 25.10 -- Korean -- \n",
      "Loss: 1.270 | Acc: 59.615% (310/520) | Conf 57.97 | time (s): 26.13 -- Latin -- \n",
      "Loss: 1.831 | Acc: 40.250% (322/800) | Conf 44.83 | time (s): 27.57 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 1.769 | Acc: 47.439% (389/820) | Conf 44.97 | time (s): 29.31 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.912 | Acc: 72.273% (477/660) | Conf 71.28 | time (s): 30.49 -- N_Ko -- \n",
      "Loss: 0.833 | Acc: 71.071% (199/280) | Conf 70.95 | time (s): 31.06 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 2.074 | Acc: 37.857% (318/840) | Conf 41.25 | time (s): 32.58 -- Sanskrit -- \n",
      "Loss: 1.601 | Acc: 53.261% (245/460) | Conf 48.73 | time (s): 33.49 -- Syriac_(Estrangelo) -- \n",
      "Loss: 1.085 | Acc: 65.588% (223/340) | Conf 67.23 | time (s): 34.13 -- Tagalog -- \n",
      "Loss: 0.996 | Acc: 69.273% (762/1100) | Conf 68.74 | time (s): 36.14 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 1.8359605874696212, TOTAL ACCURACY: 47.494813278008294 %\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 0.917 | Acc: 71.250% (285/400) | Conf 71.85 | time (s): 0.74 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.680 | Acc: 79.310% (460/580) | Conf 78.68 | time (s): 1.95 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 1.153 | Acc: 65.577% (341/520) | Conf 66.58 | time (s): 2.94 -- Arcadian -- \n",
      "Loss: 1.122 | Acc: 63.049% (517/820) | Conf 65.51 | time (s): 4.44 -- Armenian -- \n",
      "Loss: 0.745 | Acc: 75.125% (601/800) | Conf 76.29 | time (s): 5.94 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 1.840 | Acc: 45.833% (220/480) | Conf 48.95 | time (s): 6.87 -- Balinese -- \n",
      "Loss: 1.562 | Acc: 51.522% (474/920) | Conf 53.58 | time (s): 8.62 -- Bengali -- \n",
      "Loss: 0.778 | Acc: 77.143% (216/280) | Conf 78.30 | time (s): 9.20 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 1.882 | Acc: 46.154% (240/520) | Conf 35.61 | time (s): 10.16 -- Braille -- \n",
      "Loss: 1.766 | Acc: 50.735% (345/680) | Conf 55.10 | time (s): 11.59 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.940 | Acc: 71.970% (475/660) | Conf 69.41 | time (s): 12.82 -- Cyrillic -- \n",
      "Loss: 0.978 | Acc: 64.545% (284/440) | Conf 67.78 | time (s): 13.61 -- Early_Aramaic -- \n",
      "Loss: 0.687 | Acc: 77.115% (401/520) | Conf 76.03 | time (s): 14.57 -- Futurama -- \n",
      "Loss: 1.523 | Acc: 53.256% (458/860) | Conf 52.74 | time (s): 16.19 -- Grantha -- \n",
      "Loss: 0.703 | Acc: 76.042% (365/480) | Conf 76.07 | time (s): 17.04 -- Greek -- \n",
      "Loss: 1.394 | Acc: 54.896% (527/960) | Conf 59.29 | time (s): 19.01 -- Gujarati -- \n",
      "Loss: 0.900 | Acc: 69.545% (306/440) | Conf 69.75 | time (s): 19.82 -- Hebrew -- \n",
      "Loss: 0.539 | Acc: 81.875% (262/320) | Conf 79.35 | time (s): 20.62 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.844 | Acc: 75.288% (783/1040) | Conf 72.20 | time (s): 22.51 -- Japanese_(hiragana) -- \n",
      "Loss: 0.933 | Acc: 71.702% (674/940) | Conf 69.45 | time (s): 24.50 -- Japanese_(katakana) -- \n",
      "Loss: 0.754 | Acc: 74.375% (595/800) | Conf 75.34 | time (s): 25.95 -- Korean -- \n",
      "Loss: 0.755 | Acc: 75.000% (390/520) | Conf 79.32 | time (s): 27.01 -- Latin -- \n",
      "Loss: 1.338 | Acc: 55.500% (444/800) | Conf 57.26 | time (s): 28.54 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.898 | Acc: 73.049% (599/820) | Conf 68.18 | time (s): 30.28 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.374 | Acc: 86.818% (573/660) | Conf 85.18 | time (s): 31.55 -- N_Ko -- \n",
      "Loss: 0.493 | Acc: 80.714% (226/280) | Conf 81.33 | time (s): 32.11 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 1.017 | Acc: 66.786% (561/840) | Conf 66.99 | time (s): 33.63 -- Sanskrit -- \n",
      "Loss: 0.936 | Acc: 67.826% (312/460) | Conf 68.44 | time (s): 34.52 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.446 | Acc: 83.235% (283/340) | Conf 82.69 | time (s): 35.17 -- Tagalog -- \n",
      "Loss: 0.463 | Acc: 84.273% (927/1100) | Conf 83.02 | time (s): 37.43 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 1.047096950673649, TOTAL ACCURACY: 68.17427385892117 %\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 0.330 | Acc: 90.000% (360/400) | Conf 86.98 | time (s): 0.90 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.298 | Acc: 89.310% (518/580) | Conf 90.55 | time (s): 1.95 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.506 | Acc: 84.231% (438/520) | Conf 84.99 | time (s): 2.91 -- Arcadian -- \n",
      "Loss: 0.617 | Acc: 79.024% (648/820) | Conf 81.43 | time (s): 4.42 -- Armenian -- \n",
      "Loss: 0.474 | Acc: 83.625% (669/800) | Conf 87.50 | time (s): 5.85 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.953 | Acc: 70.208% (337/480) | Conf 69.33 | time (s): 6.73 -- Balinese -- \n",
      "Loss: 0.878 | Acc: 71.630% (659/920) | Conf 74.32 | time (s): 8.54 -- Bengali -- \n",
      "Loss: 0.314 | Acc: 91.071% (255/280) | Conf 90.13 | time (s): 9.26 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 1.150 | Acc: 65.385% (340/520) | Conf 59.51 | time (s): 10.32 -- Braille -- \n",
      "Loss: 1.398 | Acc: 59.559% (405/680) | Conf 67.18 | time (s): 12.11 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.470 | Acc: 85.758% (566/660) | Conf 81.43 | time (s): 13.56 -- Cyrillic -- \n",
      "Loss: 0.541 | Acc: 82.955% (365/440) | Conf 79.02 | time (s): 14.58 -- Early_Aramaic -- \n",
      "Loss: 0.345 | Acc: 89.615% (466/520) | Conf 87.96 | time (s): 15.53 -- Futurama -- \n",
      "Loss: 0.961 | Acc: 68.837% (592/860) | Conf 68.86 | time (s): 17.45 -- Grantha -- \n",
      "Loss: 0.445 | Acc: 85.625% (411/480) | Conf 84.83 | time (s): 18.38 -- Greek -- \n",
      "Loss: 0.800 | Acc: 72.500% (696/960) | Conf 75.02 | time (s): 20.06 -- Gujarati -- \n",
      "Loss: 0.535 | Acc: 82.727% (364/440) | Conf 82.15 | time (s): 21.07 -- Hebrew -- \n",
      "Loss: 0.220 | Acc: 93.125% (298/320) | Conf 90.25 | time (s): 21.63 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.425 | Acc: 86.154% (896/1040) | Conf 86.72 | time (s): 23.54 -- Japanese_(hiragana) -- \n",
      "Loss: 0.450 | Acc: 86.383% (812/940) | Conf 84.24 | time (s): 25.27 -- Japanese_(katakana) -- \n",
      "Loss: 0.310 | Acc: 89.125% (713/800) | Conf 87.15 | time (s): 26.70 -- Korean -- \n",
      "Loss: 0.408 | Acc: 86.154% (448/520) | Conf 85.59 | time (s): 27.61 -- Latin -- \n",
      "Loss: 0.900 | Acc: 68.500% (548/800) | Conf 73.87 | time (s): 29.11 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.453 | Acc: 86.829% (712/820) | Conf 82.14 | time (s): 30.71 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.181 | Acc: 94.394% (623/660) | Conf 92.47 | time (s): 31.95 -- N_Ko -- \n",
      "Loss: 0.353 | Acc: 84.286% (236/280) | Conf 87.86 | time (s): 32.54 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.606 | Acc: 78.929% (663/840) | Conf 78.03 | time (s): 34.23 -- Sanskrit -- \n",
      "Loss: 0.460 | Acc: 83.261% (383/460) | Conf 82.13 | time (s): 35.05 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.244 | Acc: 92.353% (314/340) | Conf 93.52 | time (s): 35.70 -- Tagalog -- \n",
      "Loss: 0.267 | Acc: 90.636% (997/1100) | Conf 89.67 | time (s): 37.80 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.5949362920902483, TOTAL ACCURACY: 81.59751037344398 %\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 0.266 | Acc: 92.000% (368/400) | Conf 92.41 | time (s): 0.69 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.145 | Acc: 95.345% (553/580) | Conf 93.82 | time (s): 1.83 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.216 | Acc: 92.885% (483/520) | Conf 91.64 | time (s): 2.84 -- Arcadian -- \n",
      "Loss: 0.381 | Acc: 85.854% (704/820) | Conf 87.66 | time (s): 4.38 -- Armenian -- \n",
      "Loss: 0.328 | Acc: 88.875% (711/800) | Conf 91.58 | time (s): 5.84 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.475 | Acc: 83.542% (401/480) | Conf 83.05 | time (s): 6.78 -- Balinese -- \n",
      "Loss: 0.505 | Acc: 83.261% (766/920) | Conf 86.02 | time (s): 8.51 -- Bengali -- \n",
      "Loss: 0.160 | Acc: 95.714% (268/280) | Conf 95.43 | time (s): 8.98 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.611 | Acc: 81.731% (425/520) | Conf 75.13 | time (s): 9.86 -- Braille -- \n",
      "Loss: 0.740 | Acc: 74.412% (506/680) | Conf 82.21 | time (s): 11.23 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.270 | Acc: 89.848% (593/660) | Conf 91.04 | time (s): 12.43 -- Cyrillic -- \n",
      "Loss: 0.316 | Acc: 89.318% (393/440) | Conf 89.11 | time (s): 13.25 -- Early_Aramaic -- \n",
      "Loss: 0.152 | Acc: 95.000% (494/520) | Conf 93.40 | time (s): 14.31 -- Futurama -- \n",
      "Loss: 0.593 | Acc: 81.512% (701/860) | Conf 79.57 | time (s): 15.90 -- Grantha -- \n",
      "Loss: 0.232 | Acc: 91.250% (438/480) | Conf 90.75 | time (s): 16.97 -- Greek -- \n",
      "Loss: 0.452 | Acc: 85.000% (816/960) | Conf 85.62 | time (s): 18.75 -- Gujarati -- \n",
      "Loss: 0.288 | Acc: 89.545% (394/440) | Conf 90.41 | time (s): 19.60 -- Hebrew -- \n",
      "Loss: 0.172 | Acc: 93.125% (298/320) | Conf 95.21 | time (s): 20.20 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.257 | Acc: 91.635% (953/1040) | Conf 91.59 | time (s): 22.09 -- Japanese_(hiragana) -- \n",
      "Loss: 0.170 | Acc: 94.574% (889/940) | Conf 91.91 | time (s): 23.80 -- Japanese_(katakana) -- \n",
      "Loss: 0.204 | Acc: 93.250% (746/800) | Conf 93.09 | time (s): 25.23 -- Korean -- \n",
      "Loss: 0.160 | Acc: 94.808% (493/520) | Conf 91.83 | time (s): 26.11 -- Latin -- \n",
      "Loss: 0.608 | Acc: 78.250% (626/800) | Conf 81.58 | time (s): 27.86 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.248 | Acc: 91.951% (754/820) | Conf 90.04 | time (s): 29.40 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.069 | Acc: 97.879% (646/660) | Conf 96.17 | time (s): 30.58 -- N_Ko -- \n",
      "Loss: 0.266 | Acc: 89.286% (250/280) | Conf 90.94 | time (s): 31.14 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.312 | Acc: 89.881% (755/840) | Conf 87.17 | time (s): 32.63 -- Sanskrit -- \n",
      "Loss: 0.329 | Acc: 90.000% (414/460) | Conf 88.14 | time (s): 33.49 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.156 | Acc: 94.706% (322/340) | Conf 95.08 | time (s): 34.09 -- Tagalog -- \n",
      "Loss: 0.207 | Acc: 92.727% (1020/1100) | Conf 93.11 | time (s): 35.96 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.35544355028126334, TOTAL ACCURACY: 89.10788381742739 %\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.136 | Acc: 95.250% (381/400) | Conf 95.22 | time (s): 0.72 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.193 | Acc: 94.310% (547/580) | Conf 96.36 | time (s): 1.77 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.119 | Acc: 95.962% (499/520) | Conf 94.08 | time (s): 2.72 -- Arcadian -- \n",
      "Loss: 0.281 | Acc: 90.976% (746/820) | Conf 91.68 | time (s): 4.17 -- Armenian -- \n",
      "Loss: 0.226 | Acc: 92.375% (739/800) | Conf 93.95 | time (s): 5.64 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.336 | Acc: 90.417% (434/480) | Conf 89.41 | time (s): 6.47 -- Balinese -- \n",
      "Loss: 0.283 | Acc: 90.435% (832/920) | Conf 91.33 | time (s): 8.22 -- Bengali -- \n",
      "Loss: 0.108 | Acc: 96.429% (270/280) | Conf 97.19 | time (s): 8.94 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.411 | Acc: 87.885% (457/520) | Conf 82.43 | time (s): 9.93 -- Braille -- \n",
      "Loss: 0.457 | Acc: 85.000% (578/680) | Conf 88.95 | time (s): 11.19 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.151 | Acc: 94.394% (623/660) | Conf 94.89 | time (s): 12.33 -- Cyrillic -- \n",
      "Loss: 0.187 | Acc: 94.545% (416/440) | Conf 93.99 | time (s): 13.16 -- Early_Aramaic -- \n",
      "Loss: 0.096 | Acc: 96.346% (501/520) | Conf 95.90 | time (s): 14.10 -- Futurama -- \n",
      "Loss: 0.350 | Acc: 88.837% (764/860) | Conf 86.55 | time (s): 15.68 -- Grantha -- \n",
      "Loss: 0.216 | Acc: 92.292% (443/480) | Conf 92.65 | time (s): 16.59 -- Greek -- \n",
      "Loss: 0.358 | Acc: 86.250% (828/960) | Conf 89.69 | time (s): 18.31 -- Gujarati -- \n",
      "Loss: 0.294 | Acc: 88.864% (391/440) | Conf 91.41 | time (s): 19.12 -- Hebrew -- \n",
      "Loss: 0.098 | Acc: 96.250% (308/320) | Conf 96.37 | time (s): 19.70 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.209 | Acc: 93.846% (976/1040) | Conf 94.23 | time (s): 21.52 -- Japanese_(hiragana) -- \n",
      "Loss: 0.122 | Acc: 96.277% (905/940) | Conf 93.96 | time (s): 23.26 -- Japanese_(katakana) -- \n",
      "Loss: 0.130 | Acc: 94.750% (758/800) | Conf 94.69 | time (s): 24.73 -- Korean -- \n",
      "Loss: 0.223 | Acc: 93.654% (487/520) | Conf 93.91 | time (s): 25.60 -- Latin -- \n",
      "Loss: 0.340 | Acc: 87.500% (700/800) | Conf 86.67 | time (s): 27.20 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.170 | Acc: 94.512% (775/820) | Conf 93.09 | time (s): 28.69 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.100 | Acc: 96.970% (640/660) | Conf 96.77 | time (s): 29.94 -- N_Ko -- \n",
      "Loss: 0.351 | Acc: 88.929% (249/280) | Conf 92.97 | time (s): 30.48 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.191 | Acc: 95.000% (798/840) | Conf 91.16 | time (s): 32.00 -- Sanskrit -- \n",
      "Loss: 0.206 | Acc: 92.609% (426/460) | Conf 90.91 | time (s): 32.83 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.214 | Acc: 95.294% (324/340) | Conf 97.01 | time (s): 33.42 -- Tagalog -- \n",
      "Loss: 0.125 | Acc: 95.818% (1054/1100) | Conf 95.50 | time (s): 35.70 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.25042067937791074, TOTAL ACCURACY: 92.57780082987553 %\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.095 | Acc: 97.250% (389/400) | Conf 97.48 | time (s): 0.99 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.095 | Acc: 96.897% (562/580) | Conf 96.82 | time (s): 2.07 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.052 | Acc: 98.269% (511/520) | Conf 97.27 | time (s): 3.06 -- Arcadian -- \n",
      "Loss: 0.257 | Acc: 92.439% (758/820) | Conf 93.35 | time (s): 4.56 -- Armenian -- \n",
      "Loss: 0.172 | Acc: 94.125% (753/800) | Conf 94.34 | time (s): 6.02 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.290 | Acc: 91.042% (437/480) | Conf 93.52 | time (s): 7.04 -- Balinese -- \n",
      "Loss: 0.219 | Acc: 92.065% (847/920) | Conf 93.40 | time (s): 8.74 -- Bengali -- \n",
      "Loss: 0.080 | Acc: 98.214% (275/280) | Conf 98.18 | time (s): 9.34 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.317 | Acc: 90.192% (469/520) | Conf 86.06 | time (s): 10.25 -- Braille -- \n",
      "Loss: 0.419 | Acc: 86.912% (591/680) | Conf 90.59 | time (s): 11.55 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.145 | Acc: 95.303% (629/660) | Conf 95.81 | time (s): 12.77 -- Cyrillic -- \n",
      "Loss: 0.122 | Acc: 96.364% (424/440) | Conf 96.20 | time (s): 13.65 -- Early_Aramaic -- \n",
      "Loss: 0.104 | Acc: 96.731% (503/520) | Conf 97.11 | time (s): 14.57 -- Futurama -- \n",
      "Loss: 0.238 | Acc: 90.930% (782/860) | Conf 89.52 | time (s): 16.29 -- Grantha -- \n",
      "Loss: 0.193 | Acc: 93.333% (448/480) | Conf 95.05 | time (s): 17.38 -- Greek -- \n",
      "Loss: 0.184 | Acc: 93.750% (900/960) | Conf 93.66 | time (s): 19.17 -- Gujarati -- \n",
      "Loss: 0.184 | Acc: 93.636% (412/440) | Conf 95.33 | time (s): 20.00 -- Hebrew -- \n",
      "Loss: 0.052 | Acc: 98.438% (315/320) | Conf 97.81 | time (s): 20.56 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.089 | Acc: 96.923% (1008/1040) | Conf 96.67 | time (s): 22.49 -- Japanese_(hiragana) -- \n",
      "Loss: 0.091 | Acc: 97.021% (912/940) | Conf 96.02 | time (s): 24.21 -- Japanese_(katakana) -- \n",
      "Loss: 0.136 | Acc: 95.875% (767/800) | Conf 96.01 | time (s): 25.87 -- Korean -- \n",
      "Loss: 0.116 | Acc: 96.731% (503/520) | Conf 96.20 | time (s): 26.97 -- Latin -- \n",
      "Loss: 0.235 | Acc: 92.500% (740/800) | Conf 91.33 | time (s): 28.35 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.111 | Acc: 96.220% (789/820) | Conf 95.04 | time (s): 29.95 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.073 | Acc: 97.273% (642/660) | Conf 97.48 | time (s): 31.16 -- N_Ko -- \n",
      "Loss: 0.127 | Acc: 96.071% (269/280) | Conf 95.81 | time (s): 31.70 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.177 | Acc: 93.333% (784/840) | Conf 93.46 | time (s): 33.25 -- Sanskrit -- \n",
      "Loss: 0.088 | Acc: 97.609% (449/460) | Conf 94.76 | time (s): 34.03 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.078 | Acc: 97.647% (332/340) | Conf 97.65 | time (s): 34.81 -- Tagalog -- \n",
      "Loss: 0.100 | Acc: 96.364% (1060/1100) | Conf 96.07 | time (s): 36.95 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.17646360040356535, TOTAL ACCURACY: 94.70954356846472 %\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.071 | Acc: 97.000% (388/400) | Conf 97.44 | time (s): 0.99 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.053 | Acc: 98.276% (570/580) | Conf 98.14 | time (s): 2.12 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.097 | Acc: 97.308% (506/520) | Conf 97.22 | time (s): 3.07 -- Arcadian -- \n",
      "Loss: 0.221 | Acc: 94.512% (775/820) | Conf 94.60 | time (s): 4.54 -- Armenian -- \n",
      "Loss: 0.178 | Acc: 94.750% (758/800) | Conf 96.65 | time (s): 5.87 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.239 | Acc: 93.542% (449/480) | Conf 94.78 | time (s): 6.85 -- Balinese -- \n",
      "Loss: 0.182 | Acc: 94.239% (867/920) | Conf 95.02 | time (s): 8.47 -- Bengali -- \n",
      "Loss: 0.077 | Acc: 96.786% (271/280) | Conf 97.41 | time (s): 9.00 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.182 | Acc: 94.615% (492/520) | Conf 90.29 | time (s): 9.91 -- Braille -- \n",
      "Loss: 0.291 | Acc: 91.912% (625/680) | Conf 93.98 | time (s): 11.17 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.144 | Acc: 95.303% (629/660) | Conf 96.51 | time (s): 12.43 -- Cyrillic -- \n",
      "Loss: 0.122 | Acc: 95.682% (421/440) | Conf 95.33 | time (s): 13.29 -- Early_Aramaic -- \n",
      "Loss: 0.094 | Acc: 96.538% (502/520) | Conf 96.57 | time (s): 14.17 -- Futurama -- \n",
      "Loss: 0.198 | Acc: 93.140% (801/860) | Conf 93.01 | time (s): 15.83 -- Grantha -- \n",
      "Loss: 0.086 | Acc: 96.875% (465/480) | Conf 96.84 | time (s): 16.67 -- Greek -- \n",
      "Loss: 0.264 | Acc: 91.562% (879/960) | Conf 94.26 | time (s): 18.37 -- Gujarati -- \n",
      "Loss: 0.077 | Acc: 97.273% (428/440) | Conf 96.24 | time (s): 19.20 -- Hebrew -- \n",
      "Loss: 0.079 | Acc: 96.875% (310/320) | Conf 97.86 | time (s): 19.78 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.061 | Acc: 98.173% (1021/1040) | Conf 97.63 | time (s): 21.70 -- Japanese_(hiragana) -- \n",
      "Loss: 0.109 | Acc: 96.915% (911/940) | Conf 96.79 | time (s): 23.36 -- Japanese_(katakana) -- \n",
      "Loss: 0.091 | Acc: 98.000% (784/800) | Conf 96.79 | time (s): 24.93 -- Korean -- \n",
      "Loss: 0.099 | Acc: 96.731% (503/520) | Conf 96.28 | time (s): 25.92 -- Latin -- \n",
      "Loss: 0.143 | Acc: 94.625% (757/800) | Conf 93.19 | time (s): 27.40 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.105 | Acc: 96.098% (788/820) | Conf 95.75 | time (s): 29.00 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.075 | Acc: 96.970% (640/660) | Conf 97.79 | time (s): 30.24 -- N_Ko -- \n",
      "Loss: 0.063 | Acc: 97.500% (273/280) | Conf 97.00 | time (s): 30.75 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.164 | Acc: 94.762% (796/840) | Conf 94.34 | time (s): 32.44 -- Sanskrit -- \n",
      "Loss: 0.104 | Acc: 96.304% (443/460) | Conf 95.25 | time (s): 33.24 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.055 | Acc: 98.235% (334/340) | Conf 97.63 | time (s): 34.11 -- Tagalog -- \n",
      "Loss: 0.075 | Acc: 97.545% (1073/1100) | Conf 97.42 | time (s): 36.20 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.15955373876516268, TOTAL ACCURACY: 95.74170124481329 %\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.035 | Acc: 99.000% (396/400) | Conf 97.74 | time (s): 0.92 -- Alphabet_of_the_Magi -- \n",
      "Loss: 0.058 | Acc: 97.759% (567/580) | Conf 98.26 | time (s): 1.96 -- Anglo-Saxon_Futhorc -- \n",
      "Loss: 0.049 | Acc: 98.269% (511/520) | Conf 97.80 | time (s): 2.88 -- Arcadian -- \n",
      "Loss: 0.225 | Acc: 91.707% (752/820) | Conf 94.86 | time (s): 4.32 -- Armenian -- \n",
      "Loss: 0.163 | Acc: 94.750% (758/800) | Conf 96.49 | time (s): 5.85 -- Asomtavruli_(Georgian) -- \n",
      "Loss: 0.217 | Acc: 94.792% (455/480) | Conf 95.68 | time (s): 6.67 -- Balinese -- \n",
      "Loss: 0.220 | Acc: 92.065% (847/920) | Conf 95.02 | time (s): 8.30 -- Bengali -- \n",
      "Loss: 0.038 | Acc: 98.571% (276/280) | Conf 98.30 | time (s): 8.83 -- Blackfoot_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.097 | Acc: 98.269% (511/520) | Conf 93.16 | time (s): 9.75 -- Braille -- \n",
      "Loss: 0.254 | Acc: 92.794% (631/680) | Conf 94.71 | time (s): 11.01 -- Burmese_(Myanmar) -- \n",
      "Loss: 0.106 | Acc: 96.364% (636/660) | Conf 97.10 | time (s): 12.22 -- Cyrillic -- \n",
      "Loss: 0.122 | Acc: 95.227% (419/440) | Conf 96.56 | time (s): 12.98 -- Early_Aramaic -- \n",
      "Loss: 0.037 | Acc: 99.231% (516/520) | Conf 98.59 | time (s): 14.07 -- Futurama -- \n",
      "Loss: 0.148 | Acc: 95.000% (817/860) | Conf 94.17 | time (s): 15.61 -- Grantha -- \n",
      "Loss: 0.064 | Acc: 97.917% (470/480) | Conf 98.24 | time (s): 16.61 -- Greek -- \n",
      "Loss: 0.179 | Acc: 94.062% (903/960) | Conf 95.13 | time (s): 18.32 -- Gujarati -- \n",
      "Loss: 0.115 | Acc: 95.682% (421/440) | Conf 96.60 | time (s): 19.17 -- Hebrew -- \n",
      "Loss: 0.099 | Acc: 96.875% (310/320) | Conf 97.95 | time (s): 19.85 -- Inuktitut_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.104 | Acc: 96.827% (1007/1040) | Conf 97.42 | time (s): 21.74 -- Japanese_(hiragana) -- \n",
      "Loss: 0.090 | Acc: 96.702% (909/940) | Conf 96.45 | time (s): 23.69 -- Japanese_(katakana) -- \n",
      "Loss: 0.090 | Acc: 96.500% (772/800) | Conf 97.01 | time (s): 25.34 -- Korean -- \n",
      "Loss: 0.083 | Acc: 96.538% (502/520) | Conf 96.78 | time (s): 26.38 -- Latin -- \n",
      "Loss: 0.109 | Acc: 95.625% (765/800) | Conf 95.32 | time (s): 28.15 -- Malay_(Jawi_-_Arabic) -- \n",
      "Loss: 0.111 | Acc: 95.732% (785/820) | Conf 96.29 | time (s): 29.73 -- Mkhedruli_(Georgian) -- \n",
      "Loss: 0.084 | Acc: 97.576% (644/660) | Conf 98.17 | time (s): 31.22 -- N_Ko -- \n",
      "Loss: 0.137 | Acc: 96.071% (269/280) | Conf 96.53 | time (s): 31.83 -- Ojibwe_(Canadian_Aboriginal_Syllabics) -- \n",
      "Loss: 0.177 | Acc: 94.048% (790/840) | Conf 94.37 | time (s): 33.51 -- Sanskrit -- \n",
      "Loss: 0.056 | Acc: 97.826% (450/460) | Conf 96.75 | time (s): 34.35 -- Syriac_(Estrangelo) -- \n",
      "Loss: 0.031 | Acc: 98.824% (336/340) | Conf 98.31 | time (s): 34.94 -- Tagalog -- \n",
      "Loss: 0.069 | Acc: 97.636% (1074/1100) | Conf 97.67 | time (s): 37.07 -- Tifinagh -- \n",
      "\n",
      "TOTAL LOSS: 0.14023134885138458, TOTAL ACCURACY: 95.94917012448133 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    train_loss, correct, conf = 0, 0, 0\n",
    "    total_train_loss, total_correct, total_samples = 0, 0, 0\n",
    "    start_time= time.time()\n",
    "    model.train()\n",
    "    for alphabet in alphabets:\n",
    "        train_loss, correct, conf = 0, 0, 0\n",
    "        train_loader = train_loader_dict[alphabet]\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            Y_pred = torch.nn.functional.softmax(outputs[alphabet], dim=1)\n",
    "            loss = loss_functions[alphabet](outputs[alphabet], targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            inputs.requires_grad_(False)\n",
    "            with torch.no_grad():\n",
    "                train_loss += loss.item()\n",
    "                confBatch, predicted = Y_pred.max(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                conf+=confBatch.sum().item()\n",
    "                total_samples+=batch_size\n",
    "                total_train_loss+=train_loss\n",
    "        total_correct+=correct\n",
    "        execution_time = (time.time() - start_time)\n",
    "        print(f'Loss: %.3f | Acc: %.3f%% (%d/%d) | Conf %.2f | time (s): %.2f -- {alphabet} -- '% (train_loss/len(train_loader), 100.*correct/len(train_loader.dataset), correct, len(train_loader.dataset), 100*conf/len(train_loader.dataset), execution_time))\n",
    "    print(f'\\nTOTAL LOSS: {total_train_loss/total_samples}, TOTAL ACCURACY: {(total_correct/total_samples)*100} %\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels'])\n"
     ]
    }
   ],
   "source": [
    "# load the test data:\n",
    "\n",
    "data_dict_test = load_data('test_data_task1.pkl')\n",
    "# keys are 'annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels'.\n",
    "# These keys correspond to the annotated images with known labels for each test alphabet (the sets A);\n",
    "# labels of the images with known labels for each test alphabet;\n",
    "# to-be-labeled unseen images for each test alphabet (sets U);\n",
    "# and labels of the to-be-labeled unseen images for each alphabet, respectively.\n",
    "# For each alphabet, the labels of the unseen images should be predicted by the model.\n",
    "# The true labels of the unseen images can only be used to calculate evaluation metrics.\n",
    "print(data_dict_test.keys())\n",
    "\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82021775a0a6fbf",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Mongolian annotated images: torch.Size([30, 1, 105, 105])\n",
      "Number of Mongolian annotated labels: 30\n",
      "Shape of Mongolian unseen images: torch.Size([570, 1, 105, 105])\n",
      "Number of Mongolian unseen labels: 570. Use the unseen labels only for evaluating your model!\n"
     ]
    }
   ],
   "source": [
    "# example: let's get some annotated images and their labels for an alphabet in the test data:\n",
    "\n",
    "alphabets_test = list(data_dict_test['annotated_images'].keys())\n",
    "alphabet_id = np.random.randint(0, len(alphabets_test))\n",
    "alphabet = alphabets_test[alphabet_id]\n",
    "\n",
    "alphabet_annotated = data_dict_test['annotated_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\n",
    "print(f'Shape of {alphabet} annotated images:', alphabet_annotated.shape)\n",
    "\n",
    "alphabet_annotated_labels = data_dict_test['annotated_images_labels'][alphabet]  # a list of length num_images\n",
    "print(f'Number of {alphabet} annotated labels:', len(alphabet_annotated_labels))  # equals num_images\n",
    "\n",
    "alphabet_unseen = data_dict_test['unseen_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\n",
    "print(f'Shape of {alphabet} unseen images:', alphabet_unseen.shape)\n",
    "\n",
    "alphabet_unseen_labels = data_dict_test['unseen_images_labels'][alphabet]  # a list of length num_images\n",
    "print(f'Number of {alphabet} unseen labels: {len(alphabet_unseen_labels)}. Use the unseen labels only for evaluating your model!')  # equals num_images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eedf16c955d94af7",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# evaluation of the model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1966916fdd423fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Angelic',\n 'Atemayar_Qelisayer',\n 'Atlantean',\n 'Aurek-Besh',\n 'Avesta',\n 'Ge_ez',\n 'Glagolitic',\n 'Gurmukhi',\n 'Kannada',\n 'Keble',\n 'Malayalam',\n 'Manipuri',\n 'Mongolian',\n 'Old_Church_Slavonic_(Cyrillic)',\n 'Oriya',\n 'Sylheti',\n 'Syriac_(Serto)',\n 'Tengwar',\n 'Tibetan',\n 'ULOG']"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabets_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e832c436112fef2",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e87003112448465",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a39f3d1ea11d7027",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2: rotation problem"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49a6fcc9bcd5994"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load the test data for task 2:\n",
    "# the structure of the test data of task 2 is exactly the same as for task 1,\n",
    "# but now the images are rotated by an unknown angle between 0 and 360 degrees.\n",
    "data_dict_test_task2 = load_data('test_data_task2.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "800d9fa43d711ae0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_dict_test_task2.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd690817a188882",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# solution and evaluation of task 2:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab7aa34500088f66",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71298802fa5d6fb8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6e3e82ce917c0f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e701b2a68bd4d32a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3: Domain knowledge injection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfdbe34799376c36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load the test data for task 3:\n",
    "# the structure of the data of task 3 is exactly the same as for task 1, but now our the loaded dictionary contains some additional keys.\n",
    "# These additional keys will be explained in the cells below:\n",
    "\n",
    "data_dict_test_task3 = load_data('test_data_task3.pkl')\n",
    "print(data_dict_test_task3.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa248dbece85da5c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# The keys 'annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels' are the same as for task 1, and the structure of the data is exactly the same. \n",
    "\n",
    "# The key 'unseen_images_preceding_types' maps to the type of the preceding character in the sequence where the unseen image was observed, for each alphabet.\n",
    "# The key 'character_to_type_mapping' maps to the mapping of each character to its type, for each alphabet.\n",
    "# The key 'type_following_probs' maps to the probabilities of each character type being followed by another character type, for each alphabet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb6a6237a187493",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# examples:\n",
    "\n",
    "alphabet = np.random.choice(list(data_dict_test_task3['unseen_images_preceding_types'].keys()))\n",
    "print(f'Alphabet: {alphabet}')\n",
    "\n",
    "\n",
    "preceding_character_types_alphabet = data_dict_test_task3[\"unseen_images_preceding_types\"][alphabet]  # a list\n",
    "print(f'Some character types that preceded unseen images from the {alphabet} alphabet: {np.random.choice(preceding_character_types_alphabet, size=5)}')\n",
    "print(f'There are {len(preceding_character_types_alphabet)} preceding character types in the {alphabet} alphabet, and {len(data_dict_test_task3[\"unseen_images\"][alphabet])} unseen images.')\n",
    "\n",
    "\n",
    "character_to_type_mapping_alphabet = data_dict_test_task3[\"character_to_type_mapping\"][alphabet]  \n",
    "# this is a dict, with as keys the characters and as values the types\n",
    "random_character = np.random.choice(list(character_to_type_mapping_alphabet.keys()))\n",
    "print(f'Type of {random_character} from the {alphabet} alphabet: {character_to_type_mapping_alphabet[random_character]}')\n",
    "\n",
    "\n",
    "\n",
    "type_following_probs_alphabet = data_dict_test_task3[\"type_following_probs\"][alphabet]  # a dict of dicts\n",
    "preceding_type = np.random.choice(list(type_following_probs_alphabet.keys()))\n",
    "following_type = np.random.choice(list(type_following_probs_alphabet[preceding_type].keys()))\n",
    "print(f'Probability of a character of type {following_type} following a character of type {preceding_type} in the {alphabet} alphabet: {type_following_probs_alphabet[preceding_type][following_type]}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9bcef5572f0a78",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cbaa137b41e610ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "700c29e735fd10c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb7d09f31839b40",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "596ab2a615cb44e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2656ede1e4adbe8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42d46e71207afe47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}