{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1 2AMM10 2023-2024\n",
    "\n",
    "## Group: [Fill in your group name]\n",
    "### Member 1: [Fill in your name]\n",
    "### Member 2: [Fill in your name]\n",
    "### Member 3: [Fill in your name]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc4f04c033b0e00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function for loading the training data:\n",
    "\n",
    "def load_data(file):\n",
    "    \"\"\"\n",
    "    This function loads the data from the specified pickle file and returns a dictionary with the data\n",
    "    :param filename: the pickle file\n",
    "    :return: dict with data -- keys and values differ for the train data and test data for each task.\n",
    "     Please see the cells with example code below for explanations and examples of the data structure per data set.\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19b9de0e3461531",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = load_data('train_data.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0da60a825f5080b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example alphabet names: ['Alphabet_of_the_Magi', 'Anglo-Saxon_Futhorc', 'Arcadian', 'Armenian', 'Asomtavruli_(Georgian)']\n",
      "\n",
      "\n",
      "how to get an example image for a specific character:\n",
      "shape of image 2 of character character06 of alphabet Asomtavruli_(Georgian): torch.Size([1, 105, 105])\n"
     ]
    }
   ],
   "source": [
    "# the structure of the training data is a dict, where the keys are strings indicating the alphabet.\n",
    "# The values are again dicts, with the keys being the character and the values being a list of images of that character.\n",
    "\n",
    "# see the code below for examples of working with the train data\n",
    "\n",
    "alphabets = list(train_data.keys())\n",
    "\n",
    "\n",
    "print('example alphabet names:', alphabets[:5])\n",
    "print('\\n')\n",
    "print('how to get an example image for a specific character:')\n",
    "\n",
    "alphabet_id = 4\n",
    "alphabet = alphabets[alphabet_id]  # a dict\n",
    "characters_for_this_alphabet = list(train_data[alphabet].keys())\n",
    "character_id = 5\n",
    "character = characters_for_this_alphabet[character_id]\n",
    "image_id = 2\n",
    "\n",
    "print(f'shape of image {image_id} of character {character} of alphabet {alphabet}:', train_data[alphabet][character][image_id].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289b9d9817ddf745",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2991163aba746526",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function for plotting some examples:\n",
    "\n",
    "def plot_example_data(data_dict):\n",
    "    \"\"\"\n",
    "    This function plots some examples of the data\n",
    "    :param data_dict: dict with as keys a string specifying the alphabet, and as values a dict with as keys the character of the alphabet, and as values a list om images of the alphabet\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    alphabets_to_plot = np.random.choice(list(data_dict.keys()), size=10, replace=False)\n",
    "    \n",
    "    for i, alphabet in enumerate(alphabets_to_plot):\n",
    "        characters = data_dict[alphabet]\n",
    "        character_to_plot = np.random.choice(list(characters.keys()), size=1)[0]\n",
    "        images = characters[character_to_plot]\n",
    "        im_idx = np.random.choice(len(images), size=1)[0]\n",
    "        axs[i//5, i%5].imshow(images[im_idx].permute(1, 2, 0))\n",
    "        axs[i//5, i%5].set_title(alphabet + '\\n' + character_to_plot, fontsize=8)\n",
    "        axs[i//5, i%5].axis('off')\n",
    "    # plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68bfac0812b8988",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x432 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plot_example_data(train_data)\n",
    "# plt.savefig('example_data.png', dpi=600)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "511e9fbc1b85e80f",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: character recognition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6449bef2185716"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        self.front_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "\n",
    "            nn.Linear(256 * 13 * 13, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.last_layer = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.front_layer(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f78ab6a6133991a4",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class EmbeddingNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(EmbeddingNet2, self).__init__()\n",
    "\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            # Convolutional Block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional Block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional Block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional Block 4\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Flatten\n",
    "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(13*13*512, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler\n",
    "import numpy as np\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels))\n",
    "        self.label_to_indices = {label: np.where(  np.array(self.labels) == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size <= self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16b4aff5112f09b7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "\n",
    "        anchor_idx= triplets[:, 0]\n",
    "        positive_idx= triplets[:, 1]\n",
    "        negative_idx= triplets[:, 2]\n",
    "\n",
    "\n",
    "        ap_distances = (embeddings[anchor_idx] - embeddings[positive_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[anchor_idx] - embeddings[negative_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "        print(losses)\n",
    "        return losses.mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "948e55a5ea036e8a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "char_dict = {f\"character{i:02d}\": i - 1 for i in range(1, 100)}\n",
    "train_loader_dict = {}\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets:\n",
    "    data_alphabet = train_data[alphabet]\n",
    "    image_label_list = []\n",
    "    targets = []\n",
    "    for label, images in data_alphabet.items():\n",
    "        # Append each image-label pair as a tuple to the list\n",
    "        for image in images:\n",
    "            targets.append(char_dict[label])\n",
    "            image_label_list.append((image, char_dict[label]))\n",
    "    #print(len(targets)/len(set(targets)))\n",
    "    train_batch_sampler = BalancedBatchSampler(targets, n_classes=len(set(targets)), n_samples=3)\n",
    "    triplets_train_loader = torch.utils.data.DataLoader(image_label_list, batch_sampler=train_batch_sampler)\n",
    "    train_loader_dict[alphabet] = triplets_train_loader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Batch 0:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([11, 11, 11, 18, 18, 18, 14, 14, 14,  6,  6,  6,  9,  9,  9, 17, 17, 17,\n",
      "         8,  8,  8, 24, 24, 24,  4,  4,  4, 16, 16, 16, 20, 20, 20, 25, 25, 25,\n",
      "         3,  3,  3,  2,  2,  2,  5,  5,  5, 12, 12, 12, 23, 23, 23, 10, 10, 10,\n",
      "         1,  1,  1, 22, 22, 22, 13, 13, 13, 15, 15, 15, 19, 19, 19,  0,  0,  0,\n",
      "         7,  7,  7, 21, 21, 21])\n",
      "Batch 1:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([15, 15, 15, 20, 20, 20,  7,  7,  7, 12, 12, 12,  6,  6,  6, 14, 14, 14,\n",
      "        13, 13, 13, 18, 18, 18, 17, 17, 17, 23, 23, 23,  8,  8,  8, 11, 11, 11,\n",
      "         9,  9,  9,  0,  0,  0,  4,  4,  4, 10, 10, 10, 21, 21, 21, 16, 16, 16,\n",
      "         3,  3,  3, 25, 25, 25, 22, 22, 22,  2,  2,  2,  5,  5,  5, 24, 24, 24,\n",
      "         1,  1,  1, 19, 19, 19])\n",
      "Batch 2:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([ 4,  4,  4, 18, 18, 18,  5,  5,  5,  0,  0,  0, 15, 15, 15, 23, 23, 23,\n",
      "         7,  7,  7, 21, 21, 21, 25, 25, 25, 22, 22, 22, 17, 17, 17, 24, 24, 24,\n",
      "        11, 11, 11, 16, 16, 16,  1,  1,  1, 13, 13, 13,  2,  2,  2, 10, 10, 10,\n",
      "         6,  6,  6, 20, 20, 20, 12, 12, 12, 14, 14, 14,  9,  9,  9, 19, 19, 19,\n",
      "         3,  3,  3,  8,  8,  8])\n",
      "Batch 3:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([12, 12, 12, 18, 18, 18,  0,  0,  0, 24, 24, 24,  9,  9,  9, 25, 25, 25,\n",
      "        15, 15, 15,  7,  7,  7, 16, 16, 16, 21, 21, 21,  1,  1,  1,  8,  8,  8,\n",
      "        19, 19, 19, 14, 14, 14,  3,  3,  3, 17, 17, 17, 11, 11, 11, 10, 10, 10,\n",
      "         2,  2,  2, 23, 23, 23,  5,  5,  5,  6,  6,  6, 22, 22, 22, 20, 20, 20,\n",
      "         4,  4,  4, 13, 13, 13])\n",
      "Batch 4:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([ 4,  4,  4, 12, 12, 12,  6,  6,  6, 20, 20, 20, 13, 13, 13,  5,  5,  5,\n",
      "        23, 23, 23, 11, 11, 11,  0,  0,  0, 24, 24, 24, 15, 15, 15, 16, 16, 16,\n",
      "        19, 19, 19,  8,  8,  8,  1,  1,  1, 10, 10, 10, 18, 18, 18, 14, 14, 14,\n",
      "         7,  7,  7, 17, 17, 17, 22, 22, 22, 21, 21, 21,  3,  3,  3,  9,  9,  9,\n",
      "         2,  2,  2, 25, 25, 25])\n",
      "Batch 5:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "78\n",
      "Targets (labels):\n",
      "tensor([13, 13, 13, 22, 22, 22, 17, 17, 17, 20, 20, 20,  7,  7,  7, 23, 23, 23,\n",
      "        14, 14, 14, 16, 16, 16, 10, 10, 10, 11, 11, 11,  5,  5,  5,  2,  2,  2,\n",
      "         0,  0,  0,  4,  4,  4,  3,  3,  3, 25, 25, 25,  9,  9,  9, 18, 18, 18,\n",
      "        15, 15, 15,  8,  8,  8, 12, 12, 12, 19, 19, 19,  6,  6,  6,  1,  1,  1,\n",
      "        21, 21, 21, 24, 24, 24])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader_dict['Latin']))\n",
    "i=0\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader_dict['Latin']):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Inputs (features):\")\n",
    "    print(type(inputs))\n",
    "    print(len(targets))  # Print input data (features)\n",
    "    print(\"Targets (labels):\")\n",
    "    print(targets)\n",
    "    i+=1\n",
    "print(i)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "def pdist(vectors):\n",
    "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
    "        dim=1).view(-1, 1)\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "class Informative_Negative_TripletSelector():\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(Informative_Negative_TripletSelector, self).__init__()\n",
    "\n",
    "        self.margin = margin\n",
    "\n",
    "    # Our goal is to mining informative triplets.\n",
    "    def informative_negative(self, loss_values):\n",
    "\n",
    "        informative_negative = np.where(loss_values > 0)[0]\n",
    "        return np.random.choice(informative_negative) if len(informative_negative) > 0 else None\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "\n",
    "        if torch.cuda.is_available() == False:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "        distance_matrix = distance_matrix.cpu()\n",
    "\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "            anchor_positives = np.array(anchor_positives)\n",
    "\n",
    "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
    "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
    "                loss_values = ap_distance - distance_matrix[\n",
    "                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
    "                loss_values = loss_values.data.cpu().numpy()\n",
    "\n",
    "                hard_negative = self.informative_negative(loss_values)\n",
    "                if hard_negative is not None:\n",
    "                    hard_negative = negative_indices[hard_negative]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "\n",
    "        if len(triplets) == 0:\n",
    "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
    "\n",
    "        triplets = np.array(triplets)\n",
    "        #print(len(triplets))\n",
    "        return torch.LongTensor(triplets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_dict: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_dict= training_dict\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "\n",
    "            self.model.train()  # train mode\n",
    "            alphabets = self.training_dict.keys()\n",
    "            train_losses=[]\n",
    "            for alphabet in alphabets:\n",
    "                data_loader = self.training_dict[alphabet]\n",
    "                for batch in data_loader:\n",
    "                    #print('test')\n",
    "                    x,y=batch\n",
    "                    input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "                    self.optimizer.zero_grad()  # zerograd the parameters\n",
    "                    out = self.model(input)  # one forward pass\n",
    "                    loss = self.criterion(out, target)  # calculate loss\n",
    "\n",
    "                    loss_value = loss.item()\n",
    "                    train_losses.append(loss_value)\n",
    "\n",
    "                    loss.backward()  # one backward pass\n",
    "                    self.optimizer.step()\n",
    "                    loss.cpu()\n",
    "                    input.cpu()\n",
    "                    target.cpu()\n",
    "                    del loss#update the parameters\n",
    "                    del input\n",
    "                    del target\n",
    "                #print('\\n')\n",
    "                self.model.eval()  # evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
    "            #print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-22a90683186f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;31m# start training\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun_trainer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-14-aa7d0667a7f9>\u001B[0m in \u001B[0;36mrun_trainer\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     36\u001B[0m                     \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# send to device (GPU or CPU)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# zerograd the parameters\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 38\u001B[1;33m                     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# one forward pass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     39\u001B[0m                     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# calculate loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-7-c0854b94cc22>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[1;34m\"\"\"Perform forward.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[1;31m# conv layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfront_layer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlast_layer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    177\u001B[0m             \u001B[0mbn_training\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    178\u001B[0m             \u001B[0mexponential_average_factor\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 179\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    180\u001B[0m         )\n\u001B[0;32m    181\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2281\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2282\u001B[0m     return torch.batch_norm(\n\u001B[1;32m-> 2283\u001B[1;33m         \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_mean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_var\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackends\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcudnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menabled\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2284\u001B[0m     )\n\u001B[0;32m   2285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "# model\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net.to(device)\n",
    "\n",
    "\n",
    "# margin value\n",
    "margin=1\n",
    "\n",
    "# criterion\n",
    "criterion = TripletLoss(margin,  Informative_Negative_TripletSelector(margin))\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_dict=train_loader_dict,\n",
    "                  validation_DataLoader=train_loader_dict,\n",
    "                  epochs=10)\n",
    "\n",
    "# start training\n",
    "trainer.run_trainer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name = 'test_scnn2'\n",
    "state = {'net': model.state_dict(),'loss': 1.0}\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "torch.save(state, './checkpoint/%s.t7'%(name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels'])\n"
     ]
    }
   ],
   "source": [
    "# load the test data:\n",
    "\n",
    "data_dict_test = load_data('test_data_task1.pkl')\n",
    "# keys are 'annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels'.\n",
    "# These keys correspond to the annotated images with known labels for each test alphabet (the sets A);\n",
    "# labels of the images with known labels for each test alphabet;\n",
    "# to-be-labeled unseen images for each test alphabet (sets U);\n",
    "# and labels of the to-be-labeled unseen images for each alphabet, respectively.\n",
    "# For each alphabet, the labels of the unseen images should be predicted by the model.\n",
    "# The true labels of the unseen images can only be used to calculate evaluation metrics.\n",
    "print(data_dict_test.keys())\n",
    "\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82021775a0a6fbf",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Angelic annotated images: torch.Size([20, 1, 105, 105])\n",
      "Number of Angelic annotated labels: 20\n",
      "Shape of Angelic unseen images: torch.Size([380, 1, 105, 105])\n",
      "Number of Angelic unseen labels: 380. Use the unseen labels only for evaluating your model!\n"
     ]
    }
   ],
   "source": [
    "# example: let's get some annotated images and their labels for an alphabet in the test data:\n",
    "\n",
    "alphabets_test = list(data_dict_test['annotated_images'].keys())\n",
    "alphabet_id = np.random.randint(0, len(alphabets_test))\n",
    "alphabet = alphabets_test[alphabet_id]\n",
    "\n",
    "alphabet_annotated = data_dict_test['annotated_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\n",
    "print(f'Shape of {alphabet} annotated images:', alphabet_annotated.shape)\n",
    "\n",
    "alphabet_annotated_labels = data_dict_test['annotated_images_labels'][alphabet]  # a list of length num_images\n",
    "print(f'Number of {alphabet} annotated labels:', len(alphabet_annotated_labels))  # equals num_images\n",
    "\n",
    "alphabet_unseen = data_dict_test['unseen_images'][alphabet]  # a tensor of shape (num_images, 1, height, width)\n",
    "print(f'Shape of {alphabet} unseen images:', alphabet_unseen.shape)\n",
    "\n",
    "alphabet_unseen_labels = data_dict_test['unseen_images_labels'][alphabet]  # a list of length num_images\n",
    "print(f'Number of {alphabet} unseen labels: {len(alphabet_unseen_labels)}. Use the unseen labels only for evaluating your model!')  # equals num_images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eedf16c955d94af7",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# evaluation of the model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1966916fdd423fe",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'net'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-19-b7bfbe6acc3a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0marchitecture\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mEmbeddingNet2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'embeddingNet1024.UNKNOWN'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-19-b7bfbe6acc3a>\u001B[0m in \u001B[0;36mload_net\u001B[1;34m(name, architecture, path)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mload_net\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0marchitecture\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"checkpoint/\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mcheckpoint\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmap_location\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'cpu'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0marchitecture\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'net'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[0marchitecture\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m' LOSS:\\t'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'loss'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'net'"
     ]
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "def load_net(name,architecture, path = \"checkpoint/\"):\n",
    "    checkpoint = torch.load(path+name,map_location='cpu')\n",
    "    architecture.load_state_dict(checkpoint['net'])\n",
    "    architecture.eval()\n",
    "    print(name+' LOSS:\\t',checkpoint['loss'])\n",
    "    return architecture\n",
    "model = EmbeddingNet()\n",
    "model = load_net('test1.t7', model).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddingNet1024.UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "def load_net(name,architecture, path = \"checkpoint/\"):\n",
    "    checkpoint = torch.load(path+name,map_location='cpu')\n",
    "    architecture.load_state_dict(checkpoint)\n",
    "    architecture.eval()\n",
    "    print(name)\n",
    "    return architecture\n",
    "model = EmbeddingNet2()\n",
    "model = load_net('embeddingNet1024.UNKNOWN', model).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "char_dict = {f\"character{i:02d}\": i - 1 for i in range(1, 100)}\n",
    "annotated_loader_dict = {}\n",
    "annotated_images_dict = data_dict_test['annotated_images']\n",
    "annotated_targets_dict = data_dict_test['annotated_images_labels']\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets_test:\n",
    "    images = annotated_images_dict[alphabet]\n",
    "    targets = annotated_targets_dict[alphabet]\n",
    "    targets = [char_dict[key] for key in targets]\n",
    "    annotated_loader = torch.utils.data.DataLoader(list(zip(images, targets)), batch_size=200)\n",
    "    annotated_loader_dict[alphabet] = annotated_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "char_dict = {f\"character{i:02d}\": i - 1 for i in range(1, 100)}\n",
    "test_loader_dict = {}\n",
    "test_images_dict = data_dict_test['unseen_images']\n",
    "test_targets_dict = data_dict_test['unseen_images_labels']\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets_test:\n",
    "    images = test_images_dict[alphabet]\n",
    "    targets = test_targets_dict[alphabet]\n",
    "    targets = [char_dict[key] for key in targets]\n",
    "    test_loader = torch.utils.data.DataLoader(list(zip(images, targets)), batch_size=200)\n",
    "    test_loader_dict[alphabet] = test_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "200\n",
      "Targets (labels):\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10])\n",
      "Batch 1:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "200\n",
      "Targets (labels):\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19,\n",
      "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 21])\n",
      "Batch 2:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "94\n",
      "Targets (labels):\n",
      "tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "        23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#print(len(annotated_loader_dict['Atemayar_Qelisayer']))\n",
    "i=0\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader_dict['Atemayar_Qelisayer']):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Inputs (features):\")\n",
    "    print(type(inputs))\n",
    "    print(len(targets))  # Print input data (features)\n",
    "    print(\"Targets (labels):\")\n",
    "    print(targets)\n",
    "    i+=1\n",
    "print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = list(model.children())[-1]\n",
    "last_layer.out_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def extract_embeddings(dataloader, model, out_features):\n",
    "    cuda = torch.cuda.is_available()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), out_features))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e832c436112fef2",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 287, 380, 0.7552631578947369)\n",
      "('Atemayar_Qelisayer', 550, 874, 0.6292906178489702)\n",
      "('Atlantean', 784, 1368, 0.5730994152046783)\n",
      "('Aurek-Besh', 1168, 1862, 0.6272824919441461)\n",
      "('Avesta', 1454, 2356, 0.6171477079796265)\n",
      "('Ge_ez', 1726, 2850, 0.6056140350877193)\n",
      "('Glagolitic', 2055, 3705, 0.5546558704453441)\n",
      "('Gurmukhi', 2399, 4560, 0.5260964912280702)\n",
      "('Kannada', 2657, 5339, 0.49765873759130924)\n",
      "('Keble', 3017, 5833, 0.5172295559746272)\n",
      "('Malayalam', 3412, 6726, 0.5072851620576866)\n",
      "('Manipuri', 3702, 7486, 0.4945231098049693)\n",
      "('Mongolian', 3965, 8056, 0.4921797418073486)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 4516, 8911, 0.5067893614633598)\n",
      "('Oriya', 4780, 9785, 0.4885028104241185)\n",
      "('Sylheti', 4982, 10317, 0.48289231365707086)\n",
      "('Syriac_(Serto)', 5167, 10754, 0.4804723823693509)\n",
      "('Tengwar', 5416, 11229, 0.48232255766319354)\n",
      "('Tibetan', 5818, 12027, 0.4837449072919265)\n",
      "('ULOG', 6107, 12521, 0.4877405957990576)\n",
      "0.4877405957990576\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.4877405957990576"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_results(model, annotated_loader_dict, test_loader_dict, k):\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in annotated_loader_dict.keys():\n",
    "        annotated_embeddings, annotated_targets = extract_embeddings(annotated_loader_dict[alphabet], model, out_features)\n",
    "        test_embeddings, test_targets = extract_embeddings(test_loader_dict[alphabet], model, out_features)\n",
    "        distances=cdist(annotated_embeddings,test_embeddings)\n",
    "        all_image_distances=[]\n",
    "        for i in range(len(test_targets)):\n",
    "            image_distances= []\n",
    "            for j in range(len(distances)):\n",
    "                image_distances.append((distances[j][i], j))\n",
    "            all_image_distances.append(sorted(image_distances))\n",
    "\n",
    "        k_classification = []\n",
    "        for i in range(len(all_image_distances)):\n",
    "            k_classification.append([score[1] for score in all_image_distances[i]][:k])\n",
    "        #print(all_image_distances)\n",
    "        for i in range(len(k_classification)):\n",
    "\n",
    "            if test_targets[i] in k_classification[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results(model, annotated_loader_dict, test_loader_dict, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e87003112448465",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 336, 380, 0.8842105263157894)\n",
      "('Atemayar_Qelisayer', 693, 874, 0.7929061784897025)\n",
      "('Atlantean', 980, 1368, 0.716374269005848)\n",
      "('Aurek-Besh', 1405, 1862, 0.7545649838882922)\n",
      "('Avesta', 1769, 2356, 0.7508488964346349)\n",
      "('Ge_ez', 2100, 2850, 0.7368421052631579)\n",
      "('Glagolitic', 2563, 3705, 0.6917678812415654)\n",
      "('Gurmukhi', 3054, 4560, 0.6697368421052632)\n",
      "('Kannada', 3438, 5339, 0.6439408128863083)\n",
      "('Keble', 3839, 5833, 0.6581518943939654)\n",
      "('Malayalam', 4381, 6726, 0.651352958667856)\n",
      "('Manipuri', 4772, 7486, 0.6374565856265028)\n",
      "('Mongolian', 5134, 8056, 0.6372889771598809)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 5820, 8911, 0.6531253506901582)\n",
      "('Oriya', 6163, 9785, 0.6298415942769545)\n",
      "('Sylheti', 6448, 10317, 0.624987884074828)\n",
      "('Syriac_(Serto)', 6698, 10754, 0.622838013762321)\n",
      "('Tengwar', 6995, 11229, 0.6229406002315433)\n",
      "('Tibetan', 7516, 12027, 0.6249272470275214)\n",
      "('ULOG', 7893, 12521, 0.6303809599872214)\n",
      "0.6303809599872214\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6303809599872214"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(model, annotated_loader_dict, test_loader_dict, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 360, 380, 0.9473684210526315)\n",
      "('Atemayar_Qelisayer', 785, 874, 0.8981693363844394)\n",
      "('Atlantean', 1150, 1368, 0.8406432748538012)\n",
      "('Aurek-Besh', 1593, 1862, 0.855531686358754)\n",
      "('Avesta', 2028, 2356, 0.8607809847198642)\n",
      "('Ge_ez', 2428, 2850, 0.8519298245614035)\n",
      "('Glagolitic', 3023, 3705, 0.8159244264507423)\n",
      "('Gurmukhi', 3687, 4560, 0.8085526315789474)\n",
      "('Kannada', 4188, 5339, 0.7844165574077543)\n",
      "('Keble', 4634, 5833, 0.7944453968798217)\n",
      "('Malayalam', 5322, 6726, 0.7912578055307761)\n",
      "('Manipuri', 5830, 7486, 0.7787870691958322)\n",
      "('Mongolian', 6292, 8056, 0.7810327706057597)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 7058, 8911, 0.7920547637751094)\n",
      "('Oriya', 7554, 9785, 0.7719979560551865)\n",
      "('Sylheti', 7951, 10317, 0.7706697683435108)\n",
      "('Syriac_(Serto)', 8284, 10754, 0.7703180212014135)\n",
      "('Tengwar', 8660, 11229, 0.7712173835604239)\n",
      "('Tibetan', 9335, 12027, 0.7761702835287271)\n",
      "('ULOG', 9757, 12521, 0.7792508585576232)\n",
      "0.7792508585576232\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7792508585576232"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(model, annotated_loader_dict, test_loader_dict, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 373, 380, 0.9815789473684211)\n",
      "('Atemayar_Qelisayer', 849, 874, 0.971395881006865)\n",
      "('Atlantean', 1283, 1368, 0.9378654970760234)\n",
      "('Aurek-Besh', 1752, 1862, 0.9409237379162191)\n",
      "('Avesta', 2226, 2356, 0.9448217317487266)\n",
      "('Ge_ez', 2672, 2850, 0.9375438596491228)\n",
      "('Glagolitic', 3378, 3705, 0.9117408906882591)\n",
      "('Gurmukhi', 4154, 4560, 0.9109649122807018)\n",
      "('Kannada', 4779, 5339, 0.8951114440906537)\n",
      "('Keble', 5259, 5833, 0.9015943768215327)\n",
      "('Malayalam', 6059, 6726, 0.9008325899494499)\n",
      "('Manipuri', 6676, 7486, 0.8917980229762222)\n",
      "('Mongolian', 7214, 8056, 0.8954816285998014)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 8033, 8911, 0.9014700931433061)\n",
      "('Oriya', 8689, 9785, 0.8879918242207461)\n",
      "('Sylheti', 9162, 10317, 0.8880488514102937)\n",
      "('Syriac_(Serto)', 9556, 10754, 0.8885995908499164)\n",
      "('Tengwar', 9981, 11229, 0.8888592038471814)\n",
      "('Tibetan', 10734, 12027, 0.8924918932402095)\n",
      "('ULOG', 11194, 12521, 0.8940180496765434)\n",
      "0.8940180496765434\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8940180496765434"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(model, annotated_loader_dict, test_loader_dict, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2: rotation problem"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49a6fcc9bcd5994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load the test data for task 2:\n",
    "# the structure of the test data of task 2 is exactly the same as for task 1,\n",
    "# but now the images are rotated by an unknown angle between 0 and 360 degrees.\n",
    "data_dict_test_task2 = load_data('test_data_task2.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "800d9fa43d711ae0",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels'])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_test_task2.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd690817a188882",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# solution and evaluation of task 2:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab7aa34500088f66",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71298802fa5d6fb8",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6e3e82ce917c0f",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e701b2a68bd4d32a",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3: Domain knowledge injection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfdbe34799376c36"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['unseen_images_labels', 'annotated_images_labels', 'unseen_images', 'annotated_images', 'unseen_images_preceding_types', 'character_to_type_mapping', 'type_following_probs'])\n"
     ]
    }
   ],
   "source": [
    "# load the test data for task 3:\n",
    "# the structure of the data of task 3 is exactly the same as for task 1, but now our the loaded dictionary contains some additional keys.\n",
    "# These additional keys will be explained in the cells below:\n",
    "\n",
    "data_dict_test_task3 = load_data('test_data_task3.pkl')\n",
    "print(data_dict_test_task3.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa248dbece85da5c",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# The keys 'annotated_images', 'annotated_images_labels', 'unseen_images', 'unseen_images_labels' are the same as for task 1, and the structure of the data is exactly the same. \n",
    "\n",
    "# The key 'unseen_images_preceding_types' maps to the type of the preceding character in the sequence where the unseen image was observed, for each alphabet.\n",
    "# The key 'character_to_type_mapping' maps to the mapping of each character to its type, for each alphabet.\n",
    "# The key 'type_following_probs' maps to the probabilities of each character type being followed by another character type, for each alphabet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb6a6237a187493",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: Sylheti\n",
      "Some character types that preceded unseen images from the Sylheti alphabet: ['II' 'I' 'II' 'II' 'I']\n",
      "There are 532 preceding character types in the Sylheti alphabet, and 532 unseen images.\n",
      "Type of character06 from the Sylheti alphabet: I\n",
      "Probability of a character of type I following a character of type I in the Sylheti alphabet: 0.7608695652173914\n"
     ]
    }
   ],
   "source": [
    "# examples:\n",
    "\n",
    "alphabet = np.random.choice(list(data_dict_test_task3['unseen_images_preceding_types'].keys()))\n",
    "print(f'Alphabet: {alphabet}')\n",
    "\n",
    "\n",
    "preceding_character_types_alphabet = data_dict_test_task3[\"unseen_images_preceding_types\"][alphabet]  # a list\n",
    "print(f'Some character types that preceded unseen images from the {alphabet} alphabet: {np.random.choice(preceding_character_types_alphabet, size=5)}')\n",
    "print(f'There are {len(preceding_character_types_alphabet)} preceding character types in the {alphabet} alphabet, and {len(data_dict_test_task3[\"unseen_images\"][alphabet])} unseen images.')\n",
    "\n",
    "\n",
    "character_to_type_mapping_alphabet = data_dict_test_task3[\"character_to_type_mapping\"][alphabet]  \n",
    "# this is a dict, with as keys the characters and as values the types\n",
    "random_character = np.random.choice(list(character_to_type_mapping_alphabet.keys()))\n",
    "print(f'Type of {random_character} from the {alphabet} alphabet: {character_to_type_mapping_alphabet[random_character]}')\n",
    "\n",
    "\n",
    "\n",
    "type_following_probs_alphabet = data_dict_test_task3[\"type_following_probs\"][alphabet]  # a dict of dicts\n",
    "preceding_type = np.random.choice(list(type_following_probs_alphabet.keys()))\n",
    "following_type = np.random.choice(list(type_following_probs_alphabet[preceding_type].keys()))\n",
    "print(f'Probability of a character of type {following_type} following a character of type {preceding_type} in the {alphabet} alphabet: {type_following_probs_alphabet[preceding_type][following_type]}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9bcef5572f0a78",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Angelic': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II'],\n 'Atemayar_Qelisayer': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I'],\n 'Atlantean': ['I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II'],\n 'Aurek-Besh': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Avesta': ['I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II'],\n 'Ge_ez': ['II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I'],\n 'Glagolitic': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II'],\n 'Gurmukhi': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Kannada': ['II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I'],\n 'Keble': ['I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Malayalam': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I'],\n 'Manipuri': ['I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Mongolian': ['I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II'],\n 'Old_Church_Slavonic_(Cyrillic)': ['II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II'],\n 'Oriya': ['I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Sylheti': ['I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II'],\n 'Syriac_(Serto)': ['I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II'],\n 'Tengwar': ['II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I'],\n 'Tibetan': ['II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II'],\n 'ULOG': ['I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'II',\n  'I',\n  'II',\n  'I',\n  'I',\n  'II',\n  'II',\n  'I',\n  'I',\n  'II',\n  'I',\n  'II',\n  'I',\n  'II']}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_test_task3['unseen_images_preceding_types']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbaa137b41e610ce",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "494\n",
      "855\n",
      "855\n",
      "779\n",
      "494\n",
      "893\n",
      "760\n",
      "570\n",
      "855\n",
      "874\n",
      "532\n",
      "437\n",
      "475\n",
      "798\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "char_dict = {f\"character{i:02d}\": i - 1 for i in range(1, 100)}\n",
    "char_dict_rev = {i-1:f\"character{i:02d}\" for i in range(1, 100)}\n",
    "annotated_loader_dict_task3 = {}\n",
    "annotated_images_dict_task3 = data_dict_test_task3['annotated_images']\n",
    "annotated_targets_dict_task3 = data_dict_test_task3['annotated_images_labels']\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets_test:\n",
    "    images = annotated_images_dict_task3[alphabet]\n",
    "    targets = annotated_targets_dict_task3[alphabet]\n",
    "    targets = [char_dict[key] for key in targets]\n",
    "    annotated_loader = torch.utils.data.DataLoader(list(zip(images, targets, targets)), batch_size=200)\n",
    "    annotated_loader_dict_task3[alphabet] = annotated_loader\n",
    "\n",
    "\n",
    "test_loader_dict_task3 = {}\n",
    "test_images_dict_task3 = data_dict_test_task3['unseen_images']\n",
    "test_targets_dict_task3 = data_dict_test_task3['unseen_images_labels']\n",
    "preceding_type_dict = data_dict_test_task3['unseen_images_preceding_types']\n",
    "mapping_type_dict = data_dict_test_task3['character_to_type_mapping']\n",
    "# Iterate over the dictionary items (label: images_list)\n",
    "for alphabet in alphabets_test:\n",
    "    images = test_images_dict[alphabet]\n",
    "    targets = test_targets_dict[alphabet]\n",
    "    targets = [char_dict[key] for key in targets]\n",
    "    preceding_type = preceding_type_dict[alphabet]\n",
    "    print(len(preceding_type))\n",
    "    #mapping_type = [mapping_type_dict[alphabet][char_dict_rev[target]] for target in targets]\n",
    "    test_loader = torch.utils.data.DataLoader(list(zip(images, targets, preceding_type)), batch_size=200)\n",
    "    test_loader_dict_task3[alphabet] = test_loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "700c29e735fd10c5",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'II', 'I', 'II', 'II', 'I', 'II', 'I', 'II', 'I', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'I', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'I', 'I', 'II', 'II', 'II', 'I', 'I', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'I', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'I', 'I', 'I', 'I', 'II', 'I', 'II', 'I')\n",
      "200\n",
      "Batch 0:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "200\n",
      "Targets (labels):\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10])\n",
      "('I', 'II', 'I', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'I', 'II', 'I', 'I', 'I', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'II', 'II', 'I', 'I', 'I', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'I', 'I', 'I', 'II', 'I', 'I', 'I', 'II', 'I', 'I', 'II', 'II', 'I', 'I', 'I', 'II', 'II', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'II', 'I', 'II', 'I', 'I', 'I', 'II', 'II', 'II', 'II', 'II', 'II', 'I', 'II', 'II', 'II', 'I', 'II', 'I', 'II', 'I', 'I', 'I', 'II', 'II', 'II', 'I', 'II', 'II', 'I', 'I', 'II', 'II', 'II', 'I', 'I', 'I', 'II', 'II', 'I', 'II', 'II', 'I', 'II', 'I', 'I', 'I', 'I', 'I', 'II', 'II')\n",
      "180\n",
      "Batch 1:\n",
      "Inputs (features):\n",
      "<class 'torch.Tensor'>\n",
      "180\n",
      "Targets (labels):\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19,\n",
      "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#print(len(annotated_loader_dict['Atemayar_Qelisayer']))\n",
    "i=0\n",
    "for batch_idx, (inputs, targets, p) in enumerate(test_loader_dict_task3['Angelic']):\n",
    "    print(p)\n",
    "    print(len(p))\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Inputs (features):\")\n",
    "    print(type(inputs))\n",
    "    print(len(targets))  # Print input data (features)\n",
    "    print(\"Targets (labels):\")\n",
    "    print(targets)\n",
    "    i+=1\n",
    "print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb7d09f31839b40",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'character01',\n 1: 'character02',\n 2: 'character03',\n 3: 'character04',\n 4: 'character05',\n 5: 'character06',\n 6: 'character07',\n 7: 'character08',\n 8: 'character09',\n 9: 'character10',\n 10: 'character11',\n 11: 'character12',\n 12: 'character13',\n 13: 'character14',\n 14: 'character15',\n 15: 'character16',\n 16: 'character17',\n 17: 'character18',\n 18: 'character19',\n 19: 'character20',\n 20: 'character21',\n 21: 'character22',\n 22: 'character23',\n 23: 'character24',\n 24: 'character25',\n 25: 'character26',\n 26: 'character27',\n 27: 'character28',\n 28: 'character29',\n 29: 'character30',\n 30: 'character31',\n 31: 'character32',\n 32: 'character33',\n 33: 'character34',\n 34: 'character35',\n 35: 'character36',\n 36: 'character37',\n 37: 'character38',\n 38: 'character39',\n 39: 'character40',\n 40: 'character41',\n 41: 'character42',\n 42: 'character43',\n 43: 'character44',\n 44: 'character45',\n 45: 'character46',\n 46: 'character47',\n 47: 'character48',\n 48: 'character49',\n 49: 'character50',\n 50: 'character51',\n 51: 'character52',\n 52: 'character53',\n 53: 'character54',\n 54: 'character55',\n 55: 'character56',\n 56: 'character57',\n 57: 'character58',\n 58: 'character59',\n 59: 'character60',\n 60: 'character61',\n 61: 'character62',\n 62: 'character63',\n 63: 'character64',\n 64: 'character65',\n 65: 'character66',\n 66: 'character67',\n 67: 'character68',\n 68: 'character69',\n 69: 'character70',\n 70: 'character71',\n 71: 'character72',\n 72: 'character73',\n 73: 'character74',\n 74: 'character75',\n 75: 'character76',\n 76: 'character77',\n 77: 'character78',\n 78: 'character79',\n 79: 'character80',\n 80: 'character81',\n 81: 'character82',\n 82: 'character83',\n 83: 'character84',\n 84: 'character85',\n 85: 'character86',\n 86: 'character87',\n 87: 'character88',\n 88: 'character89',\n 89: 'character90',\n 90: 'character91',\n 91: 'character92',\n 92: 'character93',\n 93: 'character94',\n 94: 'character95',\n 95: 'character96',\n 96: 'character97',\n 97: 'character98',\n 98: 'character99'}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dict_rev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Angelic': {'I': {'I': 0.2867647058823529, 'II': 0.7132352941176471},\n  'II': {'I': 0.5409836065573771, 'II': 0.45901639344262296}},\n 'Atemayar_Qelisayer': {'I': {'I': 0.0, 'II': 1.0},\n  'II': {'I': 0.8735632183908046, 'II': 0.12643678160919541}},\n 'Atlantean': {'I': {'I': 0.8113207547169812, 'II': 0.18867924528301888},\n  'II': {'I': 0.06382978723404255, 'II': 0.9361702127659575}},\n 'Aurek-Besh': {'I': {'I': 0.12096774193548387, 'II': 0.8790322580645161},\n  'II': {'I': 0.9351351351351351, 'II': 0.06486486486486487}},\n 'Avesta': {'I': {'I': 0.4, 'II': 0.6},\n  'II': {'I': 0.7990867579908676, 'II': 0.2009132420091324}},\n 'Ge_ez': {'I': {'I': 0.5808383233532934, 'II': 0.41916167664670656},\n  'II': {'I': 0.3425076452599388, 'II': 0.6574923547400612}},\n 'Glagolitic': {'I': {'I': 0.057692307692307696, 'II': 0.9423076923076923},\n  'II': {'I': 0.7311608961303462, 'II': 0.26883910386965376}},\n 'Gurmukhi': {'I': {'I': 0.8962472406181016, 'II': 0.10375275938189846},\n  'II': {'I': 0.17164179104477612, 'II': 0.8283582089552238}},\n 'Kannada': {'I': {'I': 0.7773851590106007, 'II': 0.2226148409893993},\n  'II': {'I': 0.1643192488262911, 'II': 0.8356807511737089}},\n 'Keble': {'I': {'I': 0.6237623762376238, 'II': 0.37623762376237624},\n  'II': {'I': 1.0, 'II': 0.0}},\n 'Malayalam': {'I': {'I': 0.3764172335600907, 'II': 0.6235827664399093},\n  'II': {'I': 0.5995575221238938, 'II': 0.4004424778761062}},\n 'Manipuri': {'I': {'I': 0.7532258064516129, 'II': 0.2467741935483871},\n  'II': {'I': 0.8714285714285714, 'II': 0.12857142857142856}},\n 'Mongolian': {'I': {'I': 0.0, 'II': 1.0},\n  'II': {'I': 0.5679347826086957, 'II': 0.4320652173913043}},\n 'Old_Church_Slavonic_(Cyrillic)': {'I': {'I': 0.1588447653429603,\n   'II': 0.8411552346570397},\n  'II': {'I': 0.6799307958477508, 'II': 0.32006920415224915}},\n 'Oriya': {'I': {'I': 0.6217105263157895, 'II': 0.3782894736842105},\n  'II': {'I': 0.793233082706767, 'II': 0.20676691729323307}},\n 'Sylheti': {'I': {'I': 0.7608695652173914, 'II': 0.2391304347826087},\n  'II': {'I': 0.36423841059602646, 'II': 0.6357615894039735}},\n 'Syriac_(Serto)': {'I': {'I': 0.7770700636942676, 'II': 0.2229299363057325},\n  'II': {'I': 0.9512195121951219, 'II': 0.04878048780487805}},\n 'Tengwar': {'I': {'I': 0.5041666666666667, 'II': 0.49583333333333335},\n  'II': {'I': 0.5361702127659574, 'II': 0.46382978723404256}},\n 'Tibetan': {'I': {'I': 0.9932885906040269, 'II': 0.006711409395973154},\n  'II': {'I': 0.016, 'II': 0.984}},\n 'ULOG': {'I': {'I': 0.4, 'II': 0.6},\n  'II': {'I': 0.5836431226765799, 'II': 0.4163568773234201}}}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_test_task3['type_following_probs']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 284, 380, 0.7473684210526316)\n",
      "('Atemayar_Qelisayer', 573, 874, 0.6556064073226545)\n",
      "('Atlantean', 826, 1368, 0.6038011695906432)\n",
      "('Aurek-Besh', 1210, 1862, 0.6498388829215896)\n",
      "('Avesta', 1499, 2356, 0.6362478777589134)\n",
      "('Ge_ez', 1763, 2850, 0.6185964912280701)\n",
      "('Glagolitic', 2111, 3705, 0.5697705802968961)\n",
      "('Gurmukhi', 2498, 4560, 0.5478070175438596)\n",
      "('Kannada', 2765, 5339, 0.5178872448023975)\n",
      "('Keble', 3115, 5833, 0.5340305160294874)\n",
      "('Malayalam', 3485, 6726, 0.5181385667558728)\n",
      "('Manipuri', 3738, 7486, 0.4993320865615816)\n",
      "('Mongolian', 4034, 8056, 0.5007447864945382)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 4590, 8911, 0.5150937044102795)\n",
      "('Oriya', 4842, 9785, 0.49483903934593765)\n",
      "('Sylheti', 5033, 10317, 0.4878356111272657)\n",
      "('Syriac_(Serto)', 5205, 10754, 0.4840059512739446)\n",
      "('Tengwar', 5457, 11229, 0.485973817793214)\n",
      "('Tibetan', 5997, 12027, 0.49862808680468945)\n",
      "('ULOG', 6275, 12521, 0.5011580544684929)\n",
      "0.5011580544684929\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.5011580544684929"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_embeddings_task3(dataloader, model, out_features, test=False):\n",
    "    type_dict = {'I':0, 'II':1}\n",
    "    cuda = torch.cuda.is_available()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), out_features))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        preceding_types_list = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target, preceding_type in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            if test==True:\n",
    "                preceding_type = [type_dict[i] for i in preceding_type]\n",
    "                preceding_types_list[k:k+len(images)] = preceding_type\n",
    "            k += len(images)\n",
    "    return embeddings, labels, preceding_types_list\n",
    "\n",
    "def get_results_task3(model, annotated_loader_dict, test_loader_dict, data_dict, k):\n",
    "    char_dict_rev = {i-1:f\"character{i:02d}\" for i in range(1, 100)}\n",
    "    type_dict = {0:'I', 1:'II'}\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in annotated_loader_dict.keys():\n",
    "        annotated_embeddings, annotated_targets, n = extract_embeddings_task3(annotated_loader_dict[alphabet], model, out_features)\n",
    "        test_embeddings, test_targets, preceding_types = extract_embeddings_task3(test_loader_dict[alphabet], model, out_features, test=True)\n",
    "        distances=cdist(annotated_embeddings,test_embeddings)\n",
    "        all_image_distances=[]\n",
    "        for i in range(len(test_targets)):\n",
    "            image_distances= []\n",
    "            for j in range(len(distances)):\n",
    "                character_type_mapping = data_dict['character_to_type_mapping'][alphabet][char_dict_rev[j]]\n",
    "                probability = data_dict['type_following_probs'][alphabet][type_dict[preceding_types[i]]][character_type_mapping]\n",
    "                if probability==0:\n",
    "                    probability=0.0000001\n",
    "                image_distances.append((distances[j][i] - probability,j))\n",
    "            all_image_distances.append(sorted(image_distances))\n",
    "        #print(all_image_distances)\n",
    "        k_classification = []\n",
    "        for i in range(len(all_image_distances)):\n",
    "            k_classification.append([score[1] for score in all_image_distances[i]][:k])\n",
    "        for i in range(len(k_classification)):\n",
    "            if test_targets[i] in k_classification[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results_task3(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596ab2a615cb44e9",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 332, 380, 0.8736842105263158)\n",
      "('Atemayar_Qelisayer', 730, 874, 0.8352402745995423)\n",
      "('Atlantean', 1062, 1368, 0.7763157894736842)\n",
      "('Aurek-Besh', 1491, 1862, 0.8007518796992481)\n",
      "('Avesta', 1858, 2356, 0.7886247877758913)\n",
      "('Ge_ez', 2193, 2850, 0.7694736842105263)\n",
      "('Glagolitic', 2719, 3705, 0.7338731443994602)\n",
      "('Gurmukhi', 3283, 4560, 0.7199561403508772)\n",
      "('Kannada', 3692, 5339, 0.6915152650309047)\n",
      "('Keble', 4094, 5833, 0.7018686782101834)\n",
      "('Malayalam', 4646, 6726, 0.6907523044900387)\n",
      "('Manipuri', 5016, 7486, 0.6700507614213198)\n",
      "('Mongolian', 5404, 8056, 0.6708043694141013)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 6099, 8911, 0.6844349680170576)\n",
      "('Oriya', 6444, 9785, 0.6585590189064895)\n",
      "('Sylheti', 6741, 10317, 0.6533876126781041)\n",
      "('Syriac_(Serto)', 6989, 10754, 0.6498977124790776)\n",
      "('Tengwar', 7289, 11229, 0.6491228070175439)\n",
      "('Tibetan', 7915, 12027, 0.6581026024777584)\n",
      "('ULOG', 8291, 12521, 0.6621675585017172)\n",
      "0.6621675585017172\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6621675585017172"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2656ede1e4adbe8",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 359, 380, 0.9447368421052632)\n",
      "('Atemayar_Qelisayer', 812, 874, 0.9290617848970252)\n",
      "('Atlantean', 1200, 1368, 0.8771929824561403)\n",
      "('Aurek-Besh', 1652, 1862, 0.8872180451127819)\n",
      "('Avesta', 2092, 2356, 0.8879456706281834)\n",
      "('Ge_ez', 2501, 2850, 0.8775438596491228)\n",
      "('Glagolitic', 3145, 3705, 0.8488529014844804)\n",
      "('Gurmukhi', 3837, 4560, 0.8414473684210526)\n",
      "('Kannada', 4366, 5339, 0.8177561341075108)\n",
      "('Keble', 4811, 5833, 0.8247899879993142)\n",
      "('Malayalam', 5493, 6726, 0.8166815343443354)\n",
      "('Manipuri', 5985, 7486, 0.799492385786802)\n",
      "('Mongolian', 6463, 8056, 0.8022591857000994)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 7238, 8911, 0.812254516889238)\n",
      "('Oriya', 7716, 9785, 0.7885539090444558)\n",
      "('Sylheti', 8113, 10317, 0.7863720073664825)\n",
      "('Syriac_(Serto)', 8444, 10754, 0.7851962060628603)\n",
      "('Tengwar', 8818, 11229, 0.7852880933297711)\n",
      "('Tibetan', 9541, 12027, 0.7932984119065436)\n",
      "('ULOG', 9960, 12521, 0.7954636211165242)\n",
      "0.7954636211165242\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7954636211165242"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 374, 380, 0.9842105263157894)\n",
      "('Atemayar_Qelisayer', 858, 874, 0.9816933638443935)\n",
      "('Atlantean', 1311, 1368, 0.9583333333333334)\n",
      "('Aurek-Besh', 1795, 1862, 0.9640171858216972)\n",
      "('Avesta', 2271, 2356, 0.9639219015280136)\n",
      "('Ge_ez', 2714, 2850, 0.952280701754386)\n",
      "('Glagolitic', 3451, 3705, 0.9314439946018893)\n",
      "('Gurmukhi', 4248, 4560, 0.9315789473684211)\n",
      "('Kannada', 4893, 5339, 0.9164637572579135)\n",
      "('Keble', 5370, 5833, 0.9206240356591805)\n",
      "('Malayalam', 6168, 6726, 0.9170383586083853)\n",
      "('Manipuri', 6787, 7486, 0.9066257013091104)\n",
      "('Mongolian', 7327, 8056, 0.9095084409136047)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 8150, 8911, 0.9145999326674896)\n",
      "('Oriya', 8792, 9785, 0.8985181400102197)\n",
      "('Sylheti', 9274, 10317, 0.898904720364447)\n",
      "('Syriac_(Serto)', 9671, 10754, 0.8992932862190812)\n",
      "('Tengwar', 10096, 11229, 0.8991005432362632)\n",
      "('Tibetan', 10871, 12027, 0.9038829300740001)\n",
      "('ULOG', 11332, 12521, 0.9050395335835796)\n",
      "0.9050395335835796\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9050395335835796"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'character01',\n 1: 'character02',\n 2: 'character03',\n 3: 'character04',\n 4: 'character05',\n 5: 'character06',\n 6: 'character07',\n 7: 'character08',\n 8: 'character09',\n 9: 'character10',\n 10: 'character11',\n 11: 'character12',\n 12: 'character13',\n 13: 'character14',\n 14: 'character15',\n 15: 'character16',\n 16: 'character17',\n 17: 'character18',\n 18: 'character19',\n 19: 'character20',\n 20: 'character21',\n 21: 'character22',\n 22: 'character23',\n 23: 'character24',\n 24: 'character25',\n 25: 'character26',\n 26: 'character27',\n 27: 'character28',\n 28: 'character29',\n 29: 'character30',\n 30: 'character31',\n 31: 'character32',\n 32: 'character33',\n 33: 'character34',\n 34: 'character35',\n 35: 'character36',\n 36: 'character37',\n 37: 'character38',\n 38: 'character39',\n 39: 'character40',\n 40: 'character41',\n 41: 'character42',\n 42: 'character43',\n 43: 'character44',\n 44: 'character45',\n 45: 'character46',\n 46: 'character47',\n 47: 'character48',\n 48: 'character49',\n 49: 'character50',\n 50: 'character51',\n 51: 'character52',\n 52: 'character53',\n 53: 'character54',\n 54: 'character55',\n 55: 'character56',\n 56: 'character57',\n 57: 'character58',\n 58: 'character59',\n 59: 'character60',\n 60: 'character61',\n 61: 'character62',\n 62: 'character63',\n 63: 'character64',\n 64: 'character65',\n 65: 'character66',\n 66: 'character67',\n 67: 'character68',\n 68: 'character69',\n 69: 'character70',\n 70: 'character71',\n 71: 'character72',\n 72: 'character73',\n 73: 'character74',\n 74: 'character75',\n 75: 'character76',\n 76: 'character77',\n 77: 'character78',\n 78: 'character79',\n 79: 'character80',\n 80: 'character81',\n 81: 'character82',\n 82: 'character83',\n 83: 'character84',\n 84: 'character85',\n 85: 'character86',\n 86: 'character87',\n 87: 'character88',\n 88: 'character89',\n 89: 'character90',\n 90: 'character91',\n 91: 'character92',\n 92: 'character93',\n 93: 'character94',\n 94: 'character95',\n 95: 'character96',\n 96: 'character97',\n 97: 'character98',\n 98: 'character99'}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i-1:f\"character{i:02d}\" for i in range(1, 100)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 256, 380, 0.6736842105263158)\n",
      "('Atemayar_Qelisayer', 568, 874, 0.6498855835240275)\n",
      "('Atlantean', 815, 1368, 0.5957602339181286)\n",
      "('Aurek-Besh', 1189, 1862, 0.6385606874328679)\n",
      "('Avesta', 1461, 2356, 0.620118845500849)\n",
      "('Ge_ez', 1708, 2850, 0.5992982456140351)\n",
      "('Glagolitic', 2098, 3705, 0.5662618083670715)\n",
      "('Gurmukhi', 2482, 4560, 0.5442982456140351)\n",
      "('Kannada', 2761, 5339, 0.5171380408316164)\n",
      "('Keble', 3108, 5833, 0.5328304474541402)\n",
      "('Malayalam', 3496, 6726, 0.519774011299435)\n",
      "('Manipuri', 3737, 7486, 0.49919850387389797)\n",
      "('Mongolian', 4035, 8056, 0.5008689175769613)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 4576, 8911, 0.5135226125014027)\n",
      "('Oriya', 4788, 9785, 0.4893203883495146)\n",
      "('Sylheti', 4970, 10317, 0.48172918484055444)\n",
      "('Syriac_(Serto)', 5136, 10754, 0.4775897340524456)\n",
      "('Tengwar', 5357, 11229, 0.47706830528096894)\n",
      "('Tibetan', 5857, 12027, 0.48698761120811507)\n",
      "('ULOG', 6155, 12521, 0.49157415541889626)\n",
      "0.49157415541889626\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.49157415541889626"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_results_task3_v2(model, annotated_loader_dict, test_loader_dict, data_dict, k):\n",
    "    char_dict_rev = {i-1:f\"character{i:02d}\" for i in range(1, 100)}\n",
    "    type_dict = {0:'I', 1:'II'}\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in annotated_loader_dict.keys():\n",
    "        annotated_embeddings, annotated_targets, n = extract_embeddings_task3(annotated_loader_dict[alphabet], model, out_features)\n",
    "        test_embeddings, test_targets, preceding_types = extract_embeddings_task3(test_loader_dict[alphabet], model, out_features, test=True)\n",
    "        distances=cdist(annotated_embeddings,test_embeddings)\n",
    "        all_image_distances=[]\n",
    "        for i in range(len(test_targets)):\n",
    "            image_distances= []\n",
    "            for j in range(len(distances)):\n",
    "                character_type_mapping = data_dict['character_to_type_mapping'][alphabet][char_dict_rev[j]]\n",
    "                probability = data_dict['type_following_probs'][alphabet][type_dict[preceding_types[i]]][character_type_mapping]\n",
    "                image_distances.append((distances[j][i] ,j, probability))\n",
    "            all_image_distances.append(image_distances)\n",
    "        #print(all_image_distances)\n",
    "        list_all_probabilties=[]\n",
    "        for i in range(len(all_image_distances)):\n",
    "            max_dist = max(all_image_distances[i])[0]\n",
    "            probabilities=[]\n",
    "            for j in range(len(all_image_distances[i])):\n",
    "                #print(all_image_distances[i][j][2])\n",
    "                dist_prob = (4*(1-(all_image_distances[i][j][0]/max_dist))+(all_image_distances[i][j][2]))/5\n",
    "                #print(dist_prob, (1-(all_image_distances[i][j][0]/max_dist)) )\n",
    "                probabilities.append((dist_prob, all_image_distances[i][j][1]))\n",
    "            probabilities.sort(reverse=True)\n",
    "            list_all_probabilties.append(probabilities)\n",
    "\n",
    "        #print(probabilities)\n",
    "        k_classification = []\n",
    "        for i in range(len(list_all_probabilties)):\n",
    "            k_classification.append([score[1] for score in list_all_probabilties[i]][:k])\n",
    "        #print(k_classification)\n",
    "        for i in range(len(k_classification)):\n",
    "            if test_targets[i] in k_classification[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results_task3_v2(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d46e71207afe47",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 334, 380, 0.8789473684210526)\n",
      "('Atemayar_Qelisayer', 719, 874, 0.8226544622425629)\n",
      "('Atlantean', 1049, 1368, 0.7668128654970761)\n",
      "('Aurek-Besh', 1467, 1862, 0.7878625134264232)\n",
      "('Avesta', 1828, 2356, 0.7758913412563667)\n",
      "('Ge_ez', 2163, 2850, 0.7589473684210526)\n",
      "('Glagolitic', 2669, 3705, 0.7203778677462888)\n",
      "('Gurmukhi', 3232, 4560, 0.7087719298245614)\n",
      "('Kannada', 3630, 5339, 0.6799026034837985)\n",
      "('Keble', 4024, 5833, 0.6898679924567118)\n",
      "('Malayalam', 4557, 6726, 0.6775200713648528)\n",
      "('Manipuri', 4901, 7486, 0.6546887523376971)\n",
      "('Mongolian', 5292, 8056, 0.656901688182721)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 5969, 8911, 0.6698462574346313)\n",
      "('Oriya', 6311, 9785, 0.6449667858967808)\n",
      "('Sylheti', 6602, 10317, 0.6399147038867888)\n",
      "('Syriac_(Serto)', 6844, 10754, 0.6364143574483913)\n",
      "('Tengwar', 7145, 11229, 0.636298868999911)\n",
      "('Tibetan', 7778, 12027, 0.6467115656439677)\n",
      "('ULOG', 8153, 12521, 0.651146074594681)\n",
      "0.651146074594681\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.651146074594681"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3_v2(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 357, 380, 0.9394736842105263)\n",
      "('Atemayar_Qelisayer', 799, 874, 0.914187643020595)\n",
      "('Atlantean', 1179, 1368, 0.8618421052631579)\n",
      "('Aurek-Besh', 1633, 1862, 0.8770139634801289)\n",
      "('Avesta', 2065, 2356, 0.8764855687606112)\n",
      "('Ge_ez', 2466, 2850, 0.8652631578947368)\n",
      "('Glagolitic', 3099, 3705, 0.8364372469635628)\n",
      "('Gurmukhi', 3777, 4560, 0.8282894736842106)\n",
      "('Kannada', 4289, 5339, 0.8033339576699756)\n",
      "('Keble', 4732, 5833, 0.811246356934682)\n",
      "('Malayalam', 5413, 6726, 0.8047873922093369)\n",
      "('Manipuri', 5884, 7486, 0.7860005343307508)\n",
      "('Mongolian', 6364, 8056, 0.7899702085402185)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 7124, 8911, 0.7994613399169566)\n",
      "('Oriya', 7598, 9785, 0.7764946346448646)\n",
      "('Sylheti', 7995, 10317, 0.774934574004071)\n",
      "('Syriac_(Serto)', 8315, 10754, 0.7732006695183188)\n",
      "('Tengwar', 8688, 11229, 0.7737109270638525)\n",
      "('Tibetan', 9419, 12027, 0.7831545688866717)\n",
      "('ULOG', 9839, 12521, 0.7857998562415143)\n",
      "0.7857998562415143\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7857998562415143"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3_v2(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 373, 380, 0.9815789473684211)\n",
      "('Atemayar_Qelisayer', 856, 874, 0.9794050343249427)\n",
      "('Atlantean', 1306, 1368, 0.9546783625730995)\n",
      "('Aurek-Besh', 1789, 1862, 0.9607948442534908)\n",
      "('Avesta', 2264, 2356, 0.9609507640067911)\n",
      "('Ge_ez', 2707, 2850, 0.9498245614035088)\n",
      "('Glagolitic', 3432, 3705, 0.9263157894736842)\n",
      "('Gurmukhi', 4214, 4560, 0.9241228070175439)\n",
      "('Kannada', 4846, 5339, 0.9076606106012362)\n",
      "('Keble', 5319, 5833, 0.9118806788959369)\n",
      "('Malayalam', 6118, 6726, 0.9096045197740112)\n",
      "('Manipuri', 6717, 7486, 0.897274913171253)\n",
      "('Mongolian', 7259, 8056, 0.9010675273088381)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 8074, 8911, 0.906071148019302)\n",
      "('Oriya', 8702, 9785, 0.8893203883495145)\n",
      "('Sylheti', 9180, 10317, 0.8897935446350683)\n",
      "('Syriac_(Serto)', 9574, 10754, 0.8902733866468291)\n",
      "('Tengwar', 9999, 11229, 0.8904621960993855)\n",
      "('Tibetan', 10773, 12027, 0.8957345971563981)\n",
      "('ULOG', 11234, 12521, 0.8972126826930756)\n",
      "0.8972126826930756\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8972126826930756"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_task3_v2(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 27, 380, 0.07105263157894737)\n",
      "('Atemayar_Qelisayer', 40, 874, 0.04576659038901602)\n",
      "('Atlantean', 58, 1368, 0.04239766081871345)\n",
      "('Aurek-Besh', 70, 1862, 0.03759398496240601)\n",
      "('Avesta', 89, 2356, 0.03777589134125637)\n",
      "('Ge_ez', 105, 2850, 0.03684210526315789)\n",
      "('Glagolitic', 128, 3705, 0.03454790823211876)\n",
      "('Gurmukhi', 142, 4560, 0.031140350877192982)\n",
      "('Kannada', 168, 5339, 0.0314665667728039)\n",
      "('Keble', 181, 5833, 0.03103034459111949)\n",
      "('Malayalam', 200, 6726, 0.029735355337496282)\n",
      "('Manipuri', 223, 7486, 0.029788939353459793)\n",
      "('Mongolian', 245, 8056, 0.03041211519364449)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 265, 8911, 0.02973852541802267)\n",
      "('Oriya', 282, 9785, 0.028819621870209505)\n",
      "('Sylheti', 297, 10317, 0.028787438208781622)\n",
      "('Syriac_(Serto)', 314, 10754, 0.02919843779058955)\n",
      "('Tengwar', 338, 11229, 0.03010063229138837)\n",
      "('Tibetan', 362, 12027, 0.030098944042570883)\n",
      "('ULOG', 384, 12521, 0.03066847695870937)\n",
      "0.03066847695870937\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.03066847695870937"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def get_results_random(model, test_loader_dict, k):\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in test_loader_dict.keys():\n",
    "        test_embeddings, test_targets, n = extract_embeddings_task3(test_loader_dict[alphabet], model, out_features)\n",
    "        random_pred = [random.choices(list(set(test_targets)), k=k) for i in range(len(test_targets))]\n",
    "        for i in range(len(test_targets)):\n",
    "            if test_targets[i] in random_pred[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results_random(model, test_loader_dict_task3, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 33, 380, 0.0868421052631579)\n",
      "('Atemayar_Qelisayer', 72, 874, 0.08237986270022883)\n",
      "('Atlantean', 113, 1368, 0.08260233918128655)\n",
      "('Aurek-Besh', 158, 1862, 0.08485499462943072)\n",
      "('Avesta', 202, 2356, 0.08573853989813243)\n",
      "('Ge_ez', 237, 2850, 0.08315789473684211)\n",
      "('Glagolitic', 284, 3705, 0.0766531713900135)\n",
      "('Gurmukhi', 324, 4560, 0.07105263157894737)\n",
      "('Kannada', 353, 5339, 0.06611725042142723)\n",
      "('Keble', 397, 5833, 0.0680610320589748)\n",
      "('Malayalam', 436, 6726, 0.0648230746357419)\n",
      "('Manipuri', 485, 7486, 0.06478760352658296)\n",
      "('Mongolian', 523, 8056, 0.06492055610724926)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 565, 8911, 0.06340478060823701)\n",
      "('Oriya', 604, 9785, 0.06172713336739908)\n",
      "('Sylheti', 639, 10317, 0.06193660947949985)\n",
      "('Syriac_(Serto)', 677, 10754, 0.0629533196949972)\n",
      "('Tengwar', 721, 11229, 0.06420874521328702)\n",
      "('Tibetan', 758, 12027, 0.06302486073002411)\n",
      "('ULOG', 796, 12521, 0.0635731970289913)\n",
      "0.0635731970289913\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.0635731970289913"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_random(model, test_loader_dict_task3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 58, 380, 0.15263157894736842)\n",
      "('Atemayar_Qelisayer', 131, 874, 0.14988558352402745)\n",
      "('Atlantean', 216, 1368, 0.15789473684210525)\n",
      "('Aurek-Besh', 294, 1862, 0.15789473684210525)\n",
      "('Avesta', 372, 2356, 0.15789473684210525)\n",
      "('Ge_ez', 445, 2850, 0.156140350877193)\n",
      "('Glagolitic', 523, 3705, 0.14116059379217274)\n",
      "('Gurmukhi', 612, 4560, 0.13421052631578947)\n",
      "('Kannada', 687, 5339, 0.1286757819816445)\n",
      "('Keble', 763, 5833, 0.13080747471284074)\n",
      "('Malayalam', 823, 6726, 0.1223609872137972)\n",
      "('Manipuri', 904, 7486, 0.12075874966604327)\n",
      "('Mongolian', 985, 8056, 0.12226911618669314)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 1072, 8911, 0.12030075187969924)\n",
      "('Oriya', 1153, 9785, 0.11783341849770056)\n",
      "('Sylheti', 1220, 10317, 0.1182514296791703)\n",
      "('Syriac_(Serto)', 1289, 10754, 0.11986237679003162)\n",
      "('Tengwar', 1352, 11229, 0.12040252916555348)\n",
      "('Tibetan', 1441, 12027, 0.11981375239045482)\n",
      "('ULOG', 1527, 12521, 0.12195511540611773)\n",
      "0.12195511540611773\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.12195511540611773"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_random(model, test_loader_dict_task3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 134, 380, 0.3526315789473684)\n",
      "('Atemayar_Qelisayer', 262, 874, 0.2997711670480549)\n",
      "('Atlantean', 404, 1368, 0.2953216374269006)\n",
      "('Aurek-Besh', 538, 1862, 0.2889366272824919)\n",
      "('Avesta', 660, 2356, 0.2801358234295416)\n",
      "('Ge_ez', 788, 2850, 0.27649122807017545)\n",
      "('Glagolitic', 922, 3705, 0.24885290148448044)\n",
      "('Gurmukhi', 1057, 4560, 0.2317982456140351)\n",
      "('Kannada', 1185, 5339, 0.22195167634388463)\n",
      "('Keble', 1321, 5833, 0.2264700840048003)\n",
      "('Malayalam', 1466, 6726, 0.21796015462384777)\n",
      "('Manipuri', 1616, 7486, 0.21586962329682075)\n",
      "('Mongolian', 1734, 8056, 0.21524329692154914)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 1860, 8911, 0.20873078217932892)\n",
      "('Oriya', 1988, 9785, 0.20316811446090954)\n",
      "('Sylheti', 2121, 10317, 0.20558301831927886)\n",
      "('Syriac_(Serto)', 2246, 10754, 0.2088525199925609)\n",
      "('Tengwar', 2375, 11229, 0.21150592216582065)\n",
      "('Tibetan', 2530, 12027, 0.21036002328095119)\n",
      "('ULOG', 2658, 12521, 0.2122833639485664)\n",
      "0.2122833639485664\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.2122833639485664"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_random(model, test_loader_dict_task3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 26, 380, 0.06842105263157895)\n",
      "('Atemayar_Qelisayer', 64, 874, 0.07322654462242563)\n",
      "('Atlantean', 96, 1368, 0.07017543859649122)\n",
      "('Aurek-Besh', 127, 1862, 0.0682062298603652)\n",
      "('Avesta', 153, 2356, 0.06494057724957555)\n",
      "('Ge_ez', 178, 2850, 0.062456140350877196)\n",
      "('Glagolitic', 212, 3705, 0.057219973009446694)\n",
      "('Gurmukhi', 244, 4560, 0.05350877192982456)\n",
      "('Kannada', 274, 5339, 0.05132047199850159)\n",
      "('Keble', 293, 5833, 0.050231441796674096)\n",
      "('Malayalam', 314, 6726, 0.046684507879869164)\n",
      "('Manipuri', 333, 7486, 0.04448303499866417)\n",
      "('Mongolian', 361, 8056, 0.04481132075471698)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 391, 8911, 0.043878352597912694)\n",
      "('Oriya', 410, 9785, 0.04190086867654573)\n",
      "('Sylheti', 437, 10317, 0.0423572744014733)\n",
      "('Syriac_(Serto)', 456, 10754, 0.04240282685512368)\n",
      "('Tengwar', 475, 11229, 0.04230118443316413)\n",
      "('Tibetan', 512, 12027, 0.04257088218175771)\n",
      "('ULOG', 537, 12521, 0.04288794824694513)\n",
      "0.04288794824694513\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.04288794824694513"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_results_by_type(model, annotated_loader_dict, test_loader_dict, data_dict, k):\n",
    "    char_dict_rev = {i-1:f\"character{i:02d}\" for i in range(1, 100)}\n",
    "    type_dict = {0:'I', 1:'II'}\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in annotated_loader_dict.keys():\n",
    "        annotated_embeddings, annotated_targets, n = extract_embeddings_task3(annotated_loader_dict[alphabet], model, out_features)\n",
    "        test_embeddings, test_targets, preceding_types = extract_embeddings_task3(test_loader_dict[alphabet], model, out_features, test=True)\n",
    "        distances=cdist(annotated_embeddings,test_embeddings)\n",
    "        all_image_distances=[]\n",
    "        for i in range(len(test_targets)):\n",
    "            image_distances= []\n",
    "            for j in range(len(distances)):\n",
    "                character_type_mapping = data_dict['character_to_type_mapping'][alphabet][char_dict_rev[j]]\n",
    "                probability = data_dict['type_following_probs'][alphabet][type_dict[preceding_types[i]]][character_type_mapping]\n",
    "                image_distances.append((distances[j][i] ,j, probability))\n",
    "            all_image_distances.append(image_distances)\n",
    "        #print(all_image_distances)\n",
    "        list_all_probabilties=[]\n",
    "        for i in range(len(all_image_distances)):\n",
    "            max_dist = max(all_image_distances[i])[0]\n",
    "            probabilities=[]\n",
    "            for j in range(len(all_image_distances[i])):\n",
    "                #print(all_image_distances[i][j][2])\n",
    "                dist_prob = (0*(1-(all_image_distances[i][j][0]/max_dist))+(all_image_distances[i][j][2]))\n",
    "                #print(dist_prob, (1-(all_image_distances[i][j][0]/max_dist)) )\n",
    "                probabilities.append((dist_prob, all_image_distances[i][j][1]))\n",
    "            probabilities.sort(reverse=True)\n",
    "            list_all_probabilties.append(probabilities)\n",
    "        #print(probabilities)\n",
    "        k_classification = []\n",
    "        for i in range(len(list_all_probabilties)):\n",
    "            k_classification.append([score[1] for score in list_all_probabilties[i]][:k])\n",
    "        #print(k_classification)\n",
    "        for i in range(len(k_classification)):\n",
    "            if test_targets[i] in k_classification[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results_by_type(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 49, 380, 0.12894736842105264)\n",
      "('Atemayar_Qelisayer', 123, 874, 0.14073226544622425)\n",
      "('Atlantean', 188, 1368, 0.13742690058479531)\n",
      "('Aurek-Besh', 253, 1862, 0.13587540279269603)\n",
      "('Avesta', 310, 2356, 0.13157894736842105)\n",
      "('Ge_ez', 360, 2850, 0.12631578947368421)\n",
      "('Glagolitic', 429, 3705, 0.11578947368421053)\n",
      "('Gurmukhi', 494, 4560, 0.10833333333333334)\n",
      "('Kannada', 553, 5339, 0.10357744896047949)\n",
      "('Keble', 591, 5833, 0.10132007543288188)\n",
      "('Malayalam', 636, 6726, 0.09455842997323818)\n",
      "('Manipuri', 674, 7486, 0.09003473149879776)\n",
      "('Mongolian', 728, 8056, 0.0903674280039722)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 790, 8911, 0.08865447200089777)\n",
      "('Oriya', 828, 9785, 0.08461931527848748)\n",
      "('Sylheti', 883, 10317, 0.08558689541533392)\n",
      "('Syriac_(Serto)', 921, 10754, 0.08564255160870374)\n",
      "('Tengwar', 959, 11229, 0.08540386499243031)\n",
      "('Tibetan', 1034, 12027, 0.08597322690612788)\n",
      "('ULOG', 1080, 12521, 0.0862550914463701)\n",
      "0.0862550914463701\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.0862550914463701"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_by_type(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 101, 380, 0.2657894736842105)\n",
      "('Atemayar_Qelisayer', 247, 874, 0.2826086956521739)\n",
      "('Atlantean', 382, 1368, 0.27923976608187134)\n",
      "('Aurek-Besh', 518, 1862, 0.2781954887218045)\n",
      "('Avesta', 628, 2356, 0.266553480475382)\n",
      "('Ge_ez', 730, 2850, 0.256140350877193)\n",
      "('Glagolitic', 856, 3705, 0.2310391363022942)\n",
      "('Gurmukhi', 982, 4560, 0.21535087719298246)\n",
      "('Kannada', 1099, 5339, 0.20584379097209216)\n",
      "('Keble', 1175, 5833, 0.2014400822904166)\n",
      "('Malayalam', 1269, 6726, 0.1886708296164139)\n",
      "('Manipuri', 1345, 7486, 0.17966871493454448)\n",
      "('Mongolian', 1462, 8056, 0.18147964250248264)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 1581, 8911, 0.1774211648524296)\n",
      "('Oriya', 1657, 9785, 0.16934082779764947)\n",
      "('Sylheti', 1766, 10317, 0.17117379083066783)\n",
      "('Syriac_(Serto)', 1842, 10754, 0.17128510321740748)\n",
      "('Tengwar', 1918, 11229, 0.17080772998486063)\n",
      "('Tibetan', 2069, 12027, 0.172029600066517)\n",
      "('ULOG', 2163, 12521, 0.17274978036898012)\n",
      "0.17274978036898012\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.17274978036898012"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_by_type(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 193, 380, 0.5078947368421053)\n",
      "('Atemayar_Qelisayer', 479, 874, 0.5480549199084668)\n",
      "('Atlantean', 745, 1368, 0.5445906432748538)\n",
      "('Aurek-Besh', 998, 1862, 0.5359828141783028)\n",
      "('Avesta', 1212, 2356, 0.5144312393887945)\n",
      "('Ge_ez', 1401, 2850, 0.49157894736842106)\n",
      "('Glagolitic', 1654, 3705, 0.44642375168690956)\n",
      "('Gurmukhi', 1912, 4560, 0.4192982456140351)\n",
      "('Kannada', 2140, 5339, 0.40082412436785914)\n",
      "('Keble', 2292, 5833, 0.3929367392422424)\n",
      "('Malayalam', 2473, 6726, 0.36767766874814156)\n",
      "('Manipuri', 2625, 7486, 0.35065455516965)\n",
      "('Mongolian', 2861, 8056, 0.3551390268123138)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 3088, 8911, 0.3465379867579396)\n",
      "('Oriya', 3240, 9785, 0.3311190597853858)\n",
      "('Sylheti', 3455, 10317, 0.33488417175535523)\n",
      "('Syriac_(Serto)', 3607, 10754, 0.33541007997024364)\n",
      "('Tengwar', 3759, 11229, 0.33475821533529254)\n",
      "('Tibetan', 4060, 12027, 0.33757379230065687)\n",
      "('ULOG', 4245, 12521, 0.3390304288794825)\n",
      "0.3390304288794825\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3390304288794825"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_by_type(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 31, 380, 0.08157894736842106)\n",
      "('Atemayar_Qelisayer', 75, 874, 0.08581235697940504)\n",
      "('Atlantean', 106, 1368, 0.07748538011695906)\n",
      "('Aurek-Besh', 136, 1862, 0.07303974221267455)\n",
      "('Avesta', 157, 2356, 0.0666383701188455)\n",
      "('Ge_ez', 180, 2850, 0.06315789473684211)\n",
      "('Glagolitic', 214, 3705, 0.05775978407557355)\n",
      "('Gurmukhi', 247, 4560, 0.05416666666666667)\n",
      "('Kannada', 273, 5339, 0.05113317100580633)\n",
      "('Keble', 289, 5833, 0.049545688325047146)\n",
      "('Malayalam', 317, 6726, 0.047130538209931606)\n",
      "('Manipuri', 334, 7486, 0.04461661768634785)\n",
      "('Mongolian', 364, 8056, 0.0451837140019861)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 390, 8911, 0.04376613174727864)\n",
      "('Oriya', 413, 9785, 0.042207460398569235)\n",
      "('Sylheti', 441, 10317, 0.04274498400697877)\n",
      "('Syriac_(Serto)', 458, 10754, 0.04258880416589176)\n",
      "('Tengwar', 473, 11229, 0.042123074182919225)\n",
      "('Tibetan', 511, 12027, 0.04248773592749647)\n",
      "('ULOG', 534, 12521, 0.042648350770705216)\n",
      "0.042648350770705216\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.042648350770705216"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elements_with_max_value(lst):\n",
    "    if not lst:\n",
    "        return []\n",
    "\n",
    "    max_value = max(lst)[0]\n",
    "    return [x for x in lst if round(x[0]) == round(max_value)]\n",
    "\n",
    "def get_results_by_type_v21(model, annotated_loader_dict, test_loader_dict, data_dict, k):\n",
    "    char_dict_rev = {i-1:f\"character{i:02d}\" for i in range(1, 100)}\n",
    "    type_dict = {0:'I', 1:'II'}\n",
    "    correct=0\n",
    "    total_images = 0\n",
    "    last_layer = list(model.children())[-1]\n",
    "    out_features = last_layer.out_features\n",
    "    for alphabet in annotated_loader_dict.keys():\n",
    "        annotated_embeddings, annotated_targets, n = extract_embeddings_task3(annotated_loader_dict[alphabet], model, out_features)\n",
    "        test_embeddings, test_targets, preceding_types = extract_embeddings_task3(test_loader_dict[alphabet], model, out_features, test=True)\n",
    "        distances=cdist(annotated_embeddings,test_embeddings)\n",
    "        all_image_distances=[]\n",
    "        for i in range(len(test_targets)):\n",
    "            image_distances= []\n",
    "            for j in range(len(distances)):\n",
    "                character_type_mapping = data_dict['character_to_type_mapping'][alphabet][char_dict_rev[j]]\n",
    "                probability = data_dict['type_following_probs'][alphabet][type_dict[preceding_types[i]]][character_type_mapping]\n",
    "                image_distances.append((distances[j][i] ,j, probability))\n",
    "            all_image_distances.append(image_distances)\n",
    "        #print(all_image_distances)\n",
    "        list_all_probabilties=[]\n",
    "        for i in range(len(all_image_distances)):\n",
    "            max_dist = max(all_image_distances[i])[0]\n",
    "            probabilities=[]\n",
    "            for j in range(len(all_image_distances[i])):\n",
    "                #print(all_image_distances[i][j][2])\n",
    "                dist_prob = (0*(1-(all_image_distances[i][j][0]/max_dist))+1*(all_image_distances[i][j][2]))\n",
    "                #print(dist_prob, (1-(all_image_distances[i][j][0]/max_dist)) )\n",
    "\n",
    "                probabilities.append((dist_prob, all_image_distances[i][j][1]))\n",
    "            probabilities= elements_with_max_value(probabilities)\n",
    "            random.shuffle(probabilities)\n",
    "            #print(probabilities)\n",
    "            list_all_probabilties.append(probabilities)\n",
    "        #print(probabilities)\n",
    "        k_classification = []\n",
    "        for i in range(len(list_all_probabilties)):\n",
    "            k_classification.append([score[1] for score in list_all_probabilties[i]][:k])\n",
    "            if len(k_classification[i])<k:\n",
    "                print(k_classification[i])\n",
    "        #print(k_classification)\n",
    "        for i in range(len(k_classification)):\n",
    "            if test_targets[i] in k_classification[i]:\n",
    "                correct+=1\n",
    "        total_images+=len(test_targets)\n",
    "        print((alphabet, correct, total_images, correct/total_images))\n",
    "\n",
    "    top_k_accuracy = correct/total_images\n",
    "    print(top_k_accuracy)\n",
    "    return top_k_accuracy\n",
    "get_results_by_type_v21(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Angelic', 189, 380, 0.49736842105263157)\n",
      "('Atemayar_Qelisayer', 467, 874, 0.534324942791762)\n",
      "('Atlantean', 752, 1368, 0.5497076023391813)\n",
      "[10, 16, 18, 14, 23, 15, 25]\n",
      "[18, 14, 10, 15, 23, 25, 16]\n",
      "[14, 10, 16, 25, 18, 15, 23]\n",
      "[18, 25, 14, 23, 16, 15, 10]\n",
      "[18, 10, 15, 25, 14, 23, 16]\n",
      "[15, 18, 14, 16, 25, 23, 10]\n",
      "[10, 14, 18, 25, 23, 16, 15]\n",
      "[18, 25, 23, 14, 10, 15, 16]\n",
      "[15, 10, 18, 16, 25, 14, 23]\n",
      "[16, 25, 10, 18, 15, 14, 23]\n",
      "[18, 10, 25, 16, 14, 23, 15]\n",
      "[16, 14, 18, 25, 15, 10, 23]\n",
      "[14, 25, 15, 16, 18, 10, 23]\n",
      "[18, 14, 16, 25, 23, 15, 10]\n",
      "[10, 18, 25, 14, 23, 15, 16]\n",
      "[18, 15, 25, 23, 16, 14, 10]\n",
      "[23, 18, 14, 25, 16, 10, 15]\n",
      "[18, 10, 16, 25, 14, 23, 15]\n",
      "[10, 15, 14, 23, 16, 25, 18]\n",
      "[15, 18, 25, 10, 14, 23, 16]\n",
      "[23, 16, 14, 15, 18, 10, 25]\n",
      "[10, 16, 15, 14, 23, 25, 18]\n",
      "[14, 15, 18, 25, 23, 16, 10]\n",
      "[16, 14, 10, 15, 25, 23, 18]\n",
      "[16, 14, 10, 23, 25, 15, 18]\n",
      "[14, 16, 23, 15, 25, 10, 18]\n",
      "[15, 23, 18, 14, 10, 25, 16]\n",
      "[18, 16, 23, 14, 10, 25, 15]\n",
      "[10, 16, 15, 18, 25, 14, 23]\n",
      "[23, 18, 14, 25, 10, 16, 15]\n",
      "[16, 18, 15, 14, 25, 10, 23]\n",
      "[25, 10, 14, 16, 15, 18, 23]\n",
      "[15, 23, 14, 16, 10, 18, 25]\n",
      "[18, 14, 16, 15, 25, 10, 23]\n",
      "[10, 16, 25, 23, 14, 18, 15]\n",
      "[16, 10, 14, 15, 25, 18, 23]\n",
      "[23, 18, 25, 10, 15, 14, 16]\n",
      "[23, 18, 16, 15, 25, 10, 14]\n",
      "[10, 14, 16, 23, 15, 18, 25]\n",
      "[16, 15, 10, 18, 25, 23, 14]\n",
      "[18, 14, 10, 25, 16, 15, 23]\n",
      "[10, 14, 16, 18, 15, 23, 25]\n",
      "[15, 23, 10, 18, 16, 14, 25]\n",
      "[18, 25, 14, 10, 15, 23, 16]\n",
      "[18, 25, 16, 10, 23, 14, 15]\n",
      "[23, 15, 10, 14, 25, 18, 16]\n",
      "[16, 23, 14, 25, 10, 18, 15]\n",
      "[15, 14, 23, 18, 16, 25, 10]\n",
      "[18, 25, 15, 10, 16, 14, 23]\n",
      "[10, 18, 14, 25, 15, 16, 23]\n",
      "[15, 23, 16, 25, 14, 18, 10]\n",
      "[16, 10, 18, 23, 25, 14, 15]\n",
      "[23, 15, 10, 25, 14, 18, 16]\n",
      "[23, 15, 18, 16, 10, 14, 25]\n",
      "[18, 14, 16, 10, 15, 25, 23]\n",
      "[14, 10, 25, 23, 18, 16, 15]\n",
      "[18, 25, 14, 15, 10, 16, 23]\n",
      "[23, 14, 18, 16, 10, 15, 25]\n",
      "[10, 25, 15, 14, 18, 23, 16]\n",
      "[10, 16, 15, 18, 23, 14, 25]\n",
      "[15, 23, 10, 18, 25, 16, 14]\n",
      "[10, 14, 23, 16, 18, 25, 15]\n",
      "[16, 23, 10, 15, 14, 18, 25]\n",
      "[10, 23, 18, 16, 25, 14, 15]\n",
      "[25, 16, 10, 18, 14, 23, 15]\n",
      "[16, 10, 25, 14, 15, 23, 18]\n",
      "[15, 25, 14, 18, 10, 16, 23]\n",
      "[23, 15, 16, 14, 18, 10, 25]\n",
      "[23, 15, 16, 18, 14, 25, 10]\n",
      "[14, 15, 18, 23, 25, 10, 16]\n",
      "[23, 15, 14, 10, 25, 18, 16]\n",
      "[16, 14, 18, 23, 15, 25, 10]\n",
      "[18, 14, 10, 15, 23, 16, 25]\n",
      "[14, 15, 16, 23, 25, 10, 18]\n",
      "[25, 10, 16, 14, 18, 15, 23]\n",
      "[23, 15, 14, 25, 16, 18, 10]\n",
      "[25, 14, 15, 18, 10, 16, 23]\n",
      "[23, 14, 10, 16, 25, 15, 18]\n",
      "[25, 10, 16, 23, 15, 18, 14]\n",
      "[23, 10, 15, 16, 14, 18, 25]\n",
      "[25, 18, 16, 23, 10, 14, 15]\n",
      "[25, 18, 14, 10, 23, 15, 16]\n",
      "[15, 16, 23, 10, 18, 14, 25]\n",
      "[23, 15, 25, 10, 14, 18, 16]\n",
      "[25, 14, 10, 23, 18, 15, 16]\n",
      "[10, 15, 23, 16, 14, 25, 18]\n",
      "[14, 23, 16, 18, 15, 25, 10]\n",
      "[18, 16, 10, 14, 23, 15, 25]\n",
      "[18, 10, 15, 16, 25, 14, 23]\n",
      "[18, 10, 14, 16, 23, 15, 25]\n",
      "[14, 23, 16, 18, 25, 15, 10]\n",
      "[15, 10, 14, 23, 18, 16, 25]\n",
      "[14, 18, 23, 16, 25, 15, 10]\n",
      "[25, 10, 16, 15, 23, 14, 18]\n",
      "[14, 10, 16, 15, 25, 23, 18]\n",
      "[18, 10, 16, 25, 23, 15, 14]\n",
      "[14, 25, 18, 23, 15, 16, 10]\n",
      "[23, 18, 14, 10, 25, 15, 16]\n",
      "[23, 25, 15, 16, 14, 10, 18]\n",
      "[18, 23, 16, 15, 14, 10, 25]\n",
      "[18, 25, 23, 15, 16, 10, 14]\n",
      "[15, 10, 25, 18, 23, 14, 16]\n",
      "[25, 10, 16, 18, 14, 15, 23]\n",
      "[15, 18, 16, 23, 25, 10, 14]\n",
      "[16, 14, 25, 15, 18, 23, 10]\n",
      "[16, 23, 18, 14, 10, 25, 15]\n",
      "[25, 16, 10, 14, 23, 18, 15]\n",
      "[16, 14, 18, 23, 10, 15, 25]\n",
      "[25, 16, 23, 10, 18, 15, 14]\n",
      "[16, 25, 23, 15, 18, 14, 10]\n",
      "[18, 25, 23, 10, 15, 14, 16]\n",
      "[14, 16, 10, 25, 18, 15, 23]\n",
      "[18, 23, 10, 16, 14, 25, 15]\n",
      "[16, 18, 23, 25, 10, 14, 15]\n",
      "[10, 15, 23, 25, 16, 14, 18]\n",
      "[10, 23, 16, 15, 18, 14, 25]\n",
      "[16, 25, 10, 23, 15, 18, 14]\n",
      "[16, 10, 15, 23, 25, 18, 14]\n",
      "[23, 15, 10, 14, 18, 16, 25]\n",
      "[10, 16, 25, 15, 18, 23, 14]\n",
      "[14, 23, 18, 25, 10, 15, 16]\n",
      "[25, 18, 10, 23, 14, 16, 15]\n",
      "[14, 18, 15, 25, 16, 23, 10]\n",
      "[25, 23, 10, 15, 16, 14, 18]\n",
      "('Aurek-Besh', 1020, 1862, 0.547798066595059)\n",
      "('Avesta', 1233, 2356, 0.5233446519524618)\n",
      "('Ge_ez', 1413, 2850, 0.4957894736842105)\n",
      "('Glagolitic', 1650, 3705, 0.44534412955465585)\n",
      "('Gurmukhi', 1920, 4560, 0.42105263157894735)\n",
      "('Kannada', 2153, 5339, 0.4032590372728975)\n",
      "('Keble', 2307, 5833, 0.39550831476084347)\n",
      "('Malayalam', 2495, 6726, 0.37094855783526615)\n",
      "('Manipuri', 2643, 7486, 0.3530590435479562)\n",
      "('Mongolian', 2887, 8056, 0.3583664349553128)\n",
      "('Old_Church_Slavonic_(Cyrillic)', 3098, 8911, 0.3476601952642801)\n",
      "('Oriya', 3262, 9785, 0.33336739908022484)\n",
      "('Sylheti', 3485, 10317, 0.3377919937966463)\n",
      "('Syriac_(Serto)', 3621, 10754, 0.3367119211456202)\n",
      "('Tengwar', 3774, 11229, 0.3360940422121293)\n",
      "('Tibetan', 4078, 12027, 0.3390704248773593)\n",
      "('ULOG', 4249, 12521, 0.3393498921811357)\n",
      "0.3393498921811357\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3393498921811357"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_by_type_v21(model, annotated_loader_dict_task3, test_loader_dict_task3, data_dict_test_task3, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}